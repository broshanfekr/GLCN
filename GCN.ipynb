{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNrGbcpDvYz/BhVDFLZJkj0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/broshanfekr/GNN-models/blob/main/GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import scipy.sparse as sp\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module"
      ],
      "metadata": {
        "id": "qr6G-bcdjIce"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution Layer"
      ],
      "metadata": {
        "id": "qGSRgiT60t1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output"
      ],
      "metadata": {
        "id": "f4bhB1Yi0oID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Definition:"
      ],
      "metadata": {
        "id": "O2Z6SK_A0FB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "R1VcjGy00Daz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the dataset"
      ],
      "metadata": {
        "id": "at__CiNo1IDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1406bw-_6zLyNxAuQgWM2kanYpBbl7oqS\n",
        "!mkdir ./cora\n",
        "!unzip cora.zip -d ./cora"
      ],
      "metadata": {
        "id": "gvTnLxr0h5om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Dataset"
      ],
      "metadata": {
        "id": "hRzJEuXEkTwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def load_data(path=\"./cora/\"):\n",
        "    \"\"\"Load citation network dataset\"\"\"\n",
        "    print('Loading cora dataset...')\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}cora.content\".format(path), dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}cora.cites\".format(path),\n",
        "                                    dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llfUD57ckRGA",
        "outputId": "7aa0855d-a159-4378-b242-df77c14400a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "6EnZsqi7jjfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200  # Number of epochs to train\n",
        "lr = 0.01  # Initial learning rate\n",
        "weight_decay = 5e-4  # Weight decay (L2 loss on parameters)\n",
        "hidden = 16  # Number of hidden units\n",
        "dropout = 0.5  # Dropout rate (1 - keep probability)"
      ],
      "metadata": {
        "id": "3Y21D961jjJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=hidden,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=dropout)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=lr, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "ycg04hY01T77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Evaluate validation set performance separately,\n",
        "    # deactivates dropout during validation run.\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))"
      ],
      "metadata": {
        "id": "RZO7p6qs1YNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mytest():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))"
      ],
      "metadata": {
        "id": "vy-vp_ez1a0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(epochs):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Testing\n",
        "mytest()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyWOZtPk1cem",
        "outputId": "b0bcab7a-9d67-44ef-a627-cd18e49bd018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 0.3986 acc_train: 0.9429 loss_val: 0.6999 acc_val: 0.8167 time: 0.0242s\n",
            "Epoch: 0002 loss_train: 0.4011 acc_train: 0.9429 loss_val: 0.7025 acc_val: 0.8167 time: 0.0251s\n",
            "Epoch: 0003 loss_train: 0.3948 acc_train: 0.9357 loss_val: 0.7059 acc_val: 0.8200 time: 0.0203s\n",
            "Epoch: 0004 loss_train: 0.4172 acc_train: 0.9143 loss_val: 0.7072 acc_val: 0.8200 time: 0.0198s\n",
            "Epoch: 0005 loss_train: 0.4073 acc_train: 0.9429 loss_val: 0.7083 acc_val: 0.8200 time: 0.0198s\n",
            "Epoch: 0006 loss_train: 0.3658 acc_train: 0.9571 loss_val: 0.7059 acc_val: 0.8167 time: 0.0206s\n",
            "Epoch: 0007 loss_train: 0.3768 acc_train: 0.9643 loss_val: 0.7020 acc_val: 0.8133 time: 0.0219s\n",
            "Epoch: 0008 loss_train: 0.3639 acc_train: 0.9429 loss_val: 0.6993 acc_val: 0.8133 time: 0.0217s\n",
            "Epoch: 0009 loss_train: 0.4211 acc_train: 0.9500 loss_val: 0.6984 acc_val: 0.8067 time: 0.0215s\n",
            "Epoch: 0010 loss_train: 0.3949 acc_train: 0.9429 loss_val: 0.6971 acc_val: 0.8100 time: 0.0220s\n",
            "Epoch: 0011 loss_train: 0.3732 acc_train: 0.9500 loss_val: 0.6976 acc_val: 0.8100 time: 0.0257s\n",
            "Epoch: 0012 loss_train: 0.3651 acc_train: 0.9714 loss_val: 0.6984 acc_val: 0.8100 time: 0.0220s\n",
            "Epoch: 0013 loss_train: 0.4083 acc_train: 0.9643 loss_val: 0.6996 acc_val: 0.8100 time: 0.0268s\n",
            "Epoch: 0014 loss_train: 0.4083 acc_train: 0.9143 loss_val: 0.7012 acc_val: 0.8100 time: 0.0220s\n",
            "Epoch: 0015 loss_train: 0.3878 acc_train: 0.9429 loss_val: 0.7023 acc_val: 0.8133 time: 0.0241s\n",
            "Epoch: 0016 loss_train: 0.4087 acc_train: 0.9429 loss_val: 0.7027 acc_val: 0.8100 time: 0.0240s\n",
            "Epoch: 0017 loss_train: 0.3900 acc_train: 0.9357 loss_val: 0.7016 acc_val: 0.8100 time: 0.0237s\n",
            "Epoch: 0018 loss_train: 0.3867 acc_train: 0.9571 loss_val: 0.7014 acc_val: 0.8100 time: 0.0244s\n",
            "Epoch: 0019 loss_train: 0.3650 acc_train: 0.9571 loss_val: 0.6975 acc_val: 0.8067 time: 0.0260s\n",
            "Epoch: 0020 loss_train: 0.3323 acc_train: 0.9643 loss_val: 0.6933 acc_val: 0.8100 time: 0.0252s\n",
            "Epoch: 0021 loss_train: 0.3308 acc_train: 0.9500 loss_val: 0.6895 acc_val: 0.8100 time: 0.0217s\n",
            "Epoch: 0022 loss_train: 0.3582 acc_train: 0.9571 loss_val: 0.6871 acc_val: 0.8100 time: 0.0239s\n",
            "Epoch: 0023 loss_train: 0.3703 acc_train: 0.9500 loss_val: 0.6855 acc_val: 0.8067 time: 0.0208s\n",
            "Epoch: 0024 loss_train: 0.3773 acc_train: 0.9500 loss_val: 0.6839 acc_val: 0.8033 time: 0.0264s\n",
            "Epoch: 0025 loss_train: 0.3555 acc_train: 0.9643 loss_val: 0.6844 acc_val: 0.8067 time: 0.0214s\n",
            "Epoch: 0026 loss_train: 0.3798 acc_train: 0.9357 loss_val: 0.6861 acc_val: 0.8033 time: 0.0262s\n",
            "Epoch: 0027 loss_train: 0.3688 acc_train: 0.9643 loss_val: 0.6895 acc_val: 0.8033 time: 0.0251s\n",
            "Epoch: 0028 loss_train: 0.3729 acc_train: 0.9500 loss_val: 0.6929 acc_val: 0.8067 time: 0.0215s\n",
            "Epoch: 0029 loss_train: 0.3527 acc_train: 0.9357 loss_val: 0.6942 acc_val: 0.8067 time: 0.0211s\n",
            "Epoch: 0030 loss_train: 0.3399 acc_train: 0.9571 loss_val: 0.6949 acc_val: 0.8033 time: 0.0229s\n",
            "Epoch: 0031 loss_train: 0.3627 acc_train: 0.9429 loss_val: 0.6932 acc_val: 0.8067 time: 0.0212s\n",
            "Epoch: 0032 loss_train: 0.3386 acc_train: 0.9643 loss_val: 0.6916 acc_val: 0.8067 time: 0.0237s\n",
            "Epoch: 0033 loss_train: 0.3464 acc_train: 0.9286 loss_val: 0.6905 acc_val: 0.7967 time: 0.0247s\n",
            "Epoch: 0034 loss_train: 0.3480 acc_train: 0.9714 loss_val: 0.6895 acc_val: 0.7967 time: 0.0223s\n",
            "Epoch: 0035 loss_train: 0.3592 acc_train: 0.9571 loss_val: 0.6888 acc_val: 0.7967 time: 0.0248s\n",
            "Epoch: 0036 loss_train: 0.4017 acc_train: 0.9214 loss_val: 0.6869 acc_val: 0.7967 time: 0.0227s\n",
            "Epoch: 0037 loss_train: 0.3743 acc_train: 0.9286 loss_val: 0.6837 acc_val: 0.8000 time: 0.0174s\n",
            "Epoch: 0038 loss_train: 0.3742 acc_train: 0.9429 loss_val: 0.6804 acc_val: 0.7967 time: 0.0192s\n",
            "Epoch: 0039 loss_train: 0.3609 acc_train: 0.9571 loss_val: 0.6781 acc_val: 0.8033 time: 0.0182s\n",
            "Epoch: 0040 loss_train: 0.3864 acc_train: 0.9429 loss_val: 0.6772 acc_val: 0.8033 time: 0.0171s\n",
            "Epoch: 0041 loss_train: 0.3679 acc_train: 0.9429 loss_val: 0.6771 acc_val: 0.8033 time: 0.0168s\n",
            "Epoch: 0042 loss_train: 0.3733 acc_train: 0.9500 loss_val: 0.6776 acc_val: 0.8000 time: 0.0173s\n",
            "Epoch: 0043 loss_train: 0.3645 acc_train: 0.9500 loss_val: 0.6800 acc_val: 0.7967 time: 0.0194s\n",
            "Epoch: 0044 loss_train: 0.3374 acc_train: 0.9500 loss_val: 0.6845 acc_val: 0.8000 time: 0.0199s\n",
            "Epoch: 0045 loss_train: 0.3626 acc_train: 0.9714 loss_val: 0.6902 acc_val: 0.8033 time: 0.0179s\n",
            "Epoch: 0046 loss_train: 0.3228 acc_train: 0.9429 loss_val: 0.6919 acc_val: 0.8000 time: 0.0226s\n",
            "Epoch: 0047 loss_train: 0.3690 acc_train: 0.9429 loss_val: 0.6876 acc_val: 0.8033 time: 0.0202s\n",
            "Epoch: 0048 loss_train: 0.3277 acc_train: 0.9714 loss_val: 0.6826 acc_val: 0.8000 time: 0.0196s\n",
            "Epoch: 0049 loss_train: 0.3601 acc_train: 0.9571 loss_val: 0.6775 acc_val: 0.7933 time: 0.0184s\n",
            "Epoch: 0050 loss_train: 0.3917 acc_train: 0.9357 loss_val: 0.6736 acc_val: 0.8033 time: 0.0164s\n",
            "Epoch: 0051 loss_train: 0.3708 acc_train: 0.9429 loss_val: 0.6721 acc_val: 0.8000 time: 0.0201s\n",
            "Epoch: 0052 loss_train: 0.3481 acc_train: 0.9500 loss_val: 0.6720 acc_val: 0.7967 time: 0.0171s\n",
            "Epoch: 0053 loss_train: 0.3490 acc_train: 0.9643 loss_val: 0.6743 acc_val: 0.7967 time: 0.0211s\n",
            "Epoch: 0054 loss_train: 0.3181 acc_train: 0.9500 loss_val: 0.6765 acc_val: 0.8000 time: 0.0178s\n",
            "Epoch: 0055 loss_train: 0.3340 acc_train: 0.9643 loss_val: 0.6771 acc_val: 0.8033 time: 0.0161s\n",
            "Epoch: 0056 loss_train: 0.3467 acc_train: 0.9643 loss_val: 0.6761 acc_val: 0.8067 time: 0.0149s\n",
            "Epoch: 0057 loss_train: 0.3620 acc_train: 0.9429 loss_val: 0.6751 acc_val: 0.8067 time: 0.0224s\n",
            "Epoch: 0058 loss_train: 0.3411 acc_train: 0.9643 loss_val: 0.6724 acc_val: 0.8067 time: 0.0154s\n",
            "Epoch: 0059 loss_train: 0.3520 acc_train: 0.9286 loss_val: 0.6705 acc_val: 0.8067 time: 0.0159s\n",
            "Epoch: 0060 loss_train: 0.2995 acc_train: 0.9786 loss_val: 0.6669 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0061 loss_train: 0.3593 acc_train: 0.9429 loss_val: 0.6635 acc_val: 0.8067 time: 0.0176s\n",
            "Epoch: 0062 loss_train: 0.3278 acc_train: 0.9571 loss_val: 0.6611 acc_val: 0.8067 time: 0.0182s\n",
            "Epoch: 0063 loss_train: 0.3357 acc_train: 0.9643 loss_val: 0.6598 acc_val: 0.8067 time: 0.0164s\n",
            "Epoch: 0064 loss_train: 0.3285 acc_train: 0.9571 loss_val: 0.6597 acc_val: 0.8100 time: 0.0177s\n",
            "Epoch: 0065 loss_train: 0.3150 acc_train: 0.9643 loss_val: 0.6614 acc_val: 0.8133 time: 0.0172s\n",
            "Epoch: 0066 loss_train: 0.3123 acc_train: 0.9643 loss_val: 0.6626 acc_val: 0.8100 time: 0.0204s\n",
            "Epoch: 0067 loss_train: 0.3819 acc_train: 0.9214 loss_val: 0.6642 acc_val: 0.8067 time: 0.0186s\n",
            "Epoch: 0068 loss_train: 0.3322 acc_train: 0.9357 loss_val: 0.6672 acc_val: 0.7933 time: 0.0195s\n",
            "Epoch: 0069 loss_train: 0.3311 acc_train: 0.9571 loss_val: 0.6665 acc_val: 0.8000 time: 0.0195s\n",
            "Epoch: 0070 loss_train: 0.3243 acc_train: 0.9643 loss_val: 0.6660 acc_val: 0.7967 time: 0.0226s\n",
            "Epoch: 0071 loss_train: 0.3149 acc_train: 0.9643 loss_val: 0.6624 acc_val: 0.7967 time: 0.0194s\n",
            "Epoch: 0072 loss_train: 0.3044 acc_train: 0.9571 loss_val: 0.6599 acc_val: 0.8000 time: 0.0189s\n",
            "Epoch: 0073 loss_train: 0.3405 acc_train: 0.9357 loss_val: 0.6585 acc_val: 0.8000 time: 0.0181s\n",
            "Epoch: 0074 loss_train: 0.3393 acc_train: 0.9714 loss_val: 0.6581 acc_val: 0.8000 time: 0.0177s\n",
            "Epoch: 0075 loss_train: 0.3201 acc_train: 0.9500 loss_val: 0.6589 acc_val: 0.8033 time: 0.0176s\n",
            "Epoch: 0076 loss_train: 0.3090 acc_train: 0.9643 loss_val: 0.6615 acc_val: 0.8067 time: 0.0178s\n",
            "Epoch: 0077 loss_train: 0.3200 acc_train: 0.9500 loss_val: 0.6656 acc_val: 0.8033 time: 0.0161s\n",
            "Epoch: 0078 loss_train: 0.3327 acc_train: 0.9714 loss_val: 0.6676 acc_val: 0.8000 time: 0.0174s\n",
            "Epoch: 0079 loss_train: 0.3074 acc_train: 0.9571 loss_val: 0.6690 acc_val: 0.8033 time: 0.0231s\n",
            "Epoch: 0080 loss_train: 0.3484 acc_train: 0.9643 loss_val: 0.6667 acc_val: 0.8033 time: 0.0187s\n",
            "Epoch: 0081 loss_train: 0.3082 acc_train: 0.9643 loss_val: 0.6645 acc_val: 0.8033 time: 0.0175s\n",
            "Epoch: 0082 loss_train: 0.3276 acc_train: 0.9500 loss_val: 0.6638 acc_val: 0.8033 time: 0.0201s\n",
            "Epoch: 0083 loss_train: 0.3104 acc_train: 0.9429 loss_val: 0.6637 acc_val: 0.8033 time: 0.0194s\n",
            "Epoch: 0084 loss_train: 0.3014 acc_train: 0.9500 loss_val: 0.6626 acc_val: 0.8000 time: 0.0197s\n",
            "Epoch: 0085 loss_train: 0.3147 acc_train: 0.9786 loss_val: 0.6607 acc_val: 0.7967 time: 0.0194s\n",
            "Epoch: 0086 loss_train: 0.3321 acc_train: 0.9429 loss_val: 0.6599 acc_val: 0.7933 time: 0.0179s\n",
            "Epoch: 0087 loss_train: 0.3282 acc_train: 0.9429 loss_val: 0.6600 acc_val: 0.7933 time: 0.0187s\n",
            "Epoch: 0088 loss_train: 0.3099 acc_train: 0.9500 loss_val: 0.6606 acc_val: 0.8000 time: 0.0166s\n",
            "Epoch: 0089 loss_train: 0.3305 acc_train: 0.9429 loss_val: 0.6625 acc_val: 0.7967 time: 0.0189s\n",
            "Epoch: 0090 loss_train: 0.3148 acc_train: 0.9500 loss_val: 0.6650 acc_val: 0.7933 time: 0.0233s\n",
            "Epoch: 0091 loss_train: 0.3220 acc_train: 0.9714 loss_val: 0.6670 acc_val: 0.7933 time: 0.0182s\n",
            "Epoch: 0092 loss_train: 0.2902 acc_train: 0.9857 loss_val: 0.6672 acc_val: 0.7933 time: 0.0151s\n",
            "Epoch: 0093 loss_train: 0.3211 acc_train: 0.9714 loss_val: 0.6696 acc_val: 0.7967 time: 0.0188s\n",
            "Epoch: 0094 loss_train: 0.3117 acc_train: 0.9429 loss_val: 0.6725 acc_val: 0.7967 time: 0.0196s\n",
            "Epoch: 0095 loss_train: 0.3333 acc_train: 0.9643 loss_val: 0.6730 acc_val: 0.7967 time: 0.0191s\n",
            "Epoch: 0096 loss_train: 0.2878 acc_train: 0.9571 loss_val: 0.6717 acc_val: 0.7967 time: 0.0200s\n",
            "Epoch: 0097 loss_train: 0.3369 acc_train: 0.9571 loss_val: 0.6689 acc_val: 0.7933 time: 0.0281s\n",
            "Epoch: 0098 loss_train: 0.2912 acc_train: 0.9714 loss_val: 0.6656 acc_val: 0.8000 time: 0.0178s\n",
            "Epoch: 0099 loss_train: 0.3105 acc_train: 0.9643 loss_val: 0.6625 acc_val: 0.8033 time: 0.0183s\n",
            "Epoch: 0100 loss_train: 0.3326 acc_train: 0.9714 loss_val: 0.6611 acc_val: 0.7967 time: 0.0202s\n",
            "Epoch: 0101 loss_train: 0.3099 acc_train: 0.9571 loss_val: 0.6617 acc_val: 0.8000 time: 0.0175s\n",
            "Epoch: 0102 loss_train: 0.3127 acc_train: 0.9643 loss_val: 0.6628 acc_val: 0.8033 time: 0.0180s\n",
            "Epoch: 0103 loss_train: 0.2966 acc_train: 0.9500 loss_val: 0.6632 acc_val: 0.8033 time: 0.0188s\n",
            "Epoch: 0104 loss_train: 0.3120 acc_train: 0.9571 loss_val: 0.6639 acc_val: 0.8033 time: 0.0187s\n",
            "Epoch: 0105 loss_train: 0.3067 acc_train: 0.9571 loss_val: 0.6636 acc_val: 0.7933 time: 0.0191s\n",
            "Epoch: 0106 loss_train: 0.2659 acc_train: 0.9857 loss_val: 0.6637 acc_val: 0.7900 time: 0.0161s\n",
            "Epoch: 0107 loss_train: 0.2663 acc_train: 0.9857 loss_val: 0.6641 acc_val: 0.8000 time: 0.0182s\n",
            "Epoch: 0108 loss_train: 0.3320 acc_train: 0.9500 loss_val: 0.6655 acc_val: 0.8000 time: 0.0183s\n",
            "Epoch: 0109 loss_train: 0.3063 acc_train: 0.9571 loss_val: 0.6644 acc_val: 0.8000 time: 0.0177s\n",
            "Epoch: 0110 loss_train: 0.3087 acc_train: 0.9643 loss_val: 0.6639 acc_val: 0.7967 time: 0.0181s\n",
            "Epoch: 0111 loss_train: 0.2878 acc_train: 0.9786 loss_val: 0.6629 acc_val: 0.7967 time: 0.0210s\n",
            "Epoch: 0112 loss_train: 0.2838 acc_train: 0.9714 loss_val: 0.6627 acc_val: 0.7967 time: 0.0195s\n",
            "Epoch: 0113 loss_train: 0.2643 acc_train: 0.9857 loss_val: 0.6609 acc_val: 0.8000 time: 0.0172s\n",
            "Epoch: 0114 loss_train: 0.3461 acc_train: 0.9429 loss_val: 0.6561 acc_val: 0.8033 time: 0.0170s\n",
            "Epoch: 0115 loss_train: 0.3144 acc_train: 0.9571 loss_val: 0.6514 acc_val: 0.8100 time: 0.0183s\n",
            "Epoch: 0116 loss_train: 0.3090 acc_train: 0.9571 loss_val: 0.6476 acc_val: 0.8000 time: 0.0190s\n",
            "Epoch: 0117 loss_train: 0.3247 acc_train: 0.9643 loss_val: 0.6453 acc_val: 0.7967 time: 0.0182s\n",
            "Epoch: 0118 loss_train: 0.3134 acc_train: 0.9714 loss_val: 0.6451 acc_val: 0.8033 time: 0.0165s\n",
            "Epoch: 0119 loss_train: 0.2738 acc_train: 0.9643 loss_val: 0.6478 acc_val: 0.8033 time: 0.0173s\n",
            "Epoch: 0120 loss_train: 0.3056 acc_train: 0.9500 loss_val: 0.6532 acc_val: 0.8067 time: 0.0174s\n",
            "Epoch: 0121 loss_train: 0.3001 acc_train: 0.9643 loss_val: 0.6574 acc_val: 0.8033 time: 0.0173s\n",
            "Epoch: 0122 loss_train: 0.2735 acc_train: 0.9714 loss_val: 0.6596 acc_val: 0.7967 time: 0.0212s\n",
            "Epoch: 0123 loss_train: 0.3091 acc_train: 0.9429 loss_val: 0.6593 acc_val: 0.7933 time: 0.0167s\n",
            "Epoch: 0124 loss_train: 0.2984 acc_train: 0.9500 loss_val: 0.6565 acc_val: 0.7967 time: 0.0190s\n",
            "Epoch: 0125 loss_train: 0.3137 acc_train: 0.9429 loss_val: 0.6527 acc_val: 0.7933 time: 0.0185s\n",
            "Epoch: 0126 loss_train: 0.2814 acc_train: 0.9643 loss_val: 0.6497 acc_val: 0.8033 time: 0.0183s\n",
            "Epoch: 0127 loss_train: 0.2940 acc_train: 0.9571 loss_val: 0.6479 acc_val: 0.8067 time: 0.0178s\n",
            "Epoch: 0128 loss_train: 0.3170 acc_train: 0.9429 loss_val: 0.6476 acc_val: 0.8067 time: 0.0191s\n",
            "Epoch: 0129 loss_train: 0.3137 acc_train: 0.9786 loss_val: 0.6464 acc_val: 0.8067 time: 0.0191s\n",
            "Epoch: 0130 loss_train: 0.2884 acc_train: 0.9429 loss_val: 0.6462 acc_val: 0.8000 time: 0.0185s\n",
            "Epoch: 0131 loss_train: 0.3079 acc_train: 0.9500 loss_val: 0.6496 acc_val: 0.8033 time: 0.0181s\n",
            "Epoch: 0132 loss_train: 0.2784 acc_train: 0.9643 loss_val: 0.6554 acc_val: 0.8000 time: 0.0185s\n",
            "Epoch: 0133 loss_train: 0.3031 acc_train: 0.9571 loss_val: 0.6619 acc_val: 0.8000 time: 0.0205s\n",
            "Epoch: 0134 loss_train: 0.2862 acc_train: 0.9571 loss_val: 0.6665 acc_val: 0.8033 time: 0.0166s\n",
            "Epoch: 0135 loss_train: 0.2547 acc_train: 0.9857 loss_val: 0.6657 acc_val: 0.8033 time: 0.0180s\n",
            "Epoch: 0136 loss_train: 0.3521 acc_train: 0.9643 loss_val: 0.6601 acc_val: 0.7967 time: 0.0211s\n",
            "Epoch: 0137 loss_train: 0.2780 acc_train: 0.9643 loss_val: 0.6541 acc_val: 0.8000 time: 0.0168s\n",
            "Epoch: 0138 loss_train: 0.2920 acc_train: 0.9714 loss_val: 0.6489 acc_val: 0.7967 time: 0.0187s\n",
            "Epoch: 0139 loss_train: 0.2772 acc_train: 0.9643 loss_val: 0.6461 acc_val: 0.8067 time: 0.0186s\n",
            "Epoch: 0140 loss_train: 0.2968 acc_train: 0.9500 loss_val: 0.6466 acc_val: 0.8067 time: 0.0179s\n",
            "Epoch: 0141 loss_train: 0.3137 acc_train: 0.9500 loss_val: 0.6487 acc_val: 0.8033 time: 0.0174s\n",
            "Epoch: 0142 loss_train: 0.3117 acc_train: 0.9429 loss_val: 0.6513 acc_val: 0.8033 time: 0.0191s\n",
            "Epoch: 0143 loss_train: 0.2927 acc_train: 0.9714 loss_val: 0.6544 acc_val: 0.8033 time: 0.0202s\n",
            "Epoch: 0144 loss_train: 0.2665 acc_train: 0.9714 loss_val: 0.6579 acc_val: 0.7967 time: 0.0211s\n",
            "Epoch: 0145 loss_train: 0.2685 acc_train: 0.9857 loss_val: 0.6609 acc_val: 0.8033 time: 0.0187s\n",
            "Epoch: 0146 loss_train: 0.2671 acc_train: 0.9714 loss_val: 0.6650 acc_val: 0.8000 time: 0.0190s\n",
            "Epoch: 0147 loss_train: 0.2857 acc_train: 0.9714 loss_val: 0.6669 acc_val: 0.8033 time: 0.0179s\n",
            "Epoch: 0148 loss_train: 0.3023 acc_train: 0.9357 loss_val: 0.6655 acc_val: 0.8033 time: 0.0177s\n",
            "Epoch: 0149 loss_train: 0.2772 acc_train: 0.9643 loss_val: 0.6582 acc_val: 0.8000 time: 0.0244s\n",
            "Epoch: 0150 loss_train: 0.2875 acc_train: 0.9571 loss_val: 0.6493 acc_val: 0.8000 time: 0.0185s\n",
            "Epoch: 0151 loss_train: 0.3073 acc_train: 0.9429 loss_val: 0.6436 acc_val: 0.8000 time: 0.0193s\n",
            "Epoch: 0152 loss_train: 0.2585 acc_train: 0.9714 loss_val: 0.6422 acc_val: 0.8067 time: 0.0196s\n",
            "Epoch: 0153 loss_train: 0.2604 acc_train: 0.9643 loss_val: 0.6430 acc_val: 0.8033 time: 0.0178s\n",
            "Epoch: 0154 loss_train: 0.2849 acc_train: 0.9643 loss_val: 0.6439 acc_val: 0.8000 time: 0.0175s\n",
            "Epoch: 0155 loss_train: 0.2500 acc_train: 0.9714 loss_val: 0.6446 acc_val: 0.8033 time: 0.0224s\n",
            "Epoch: 0156 loss_train: 0.2627 acc_train: 0.9643 loss_val: 0.6470 acc_val: 0.7967 time: 0.0193s\n",
            "Epoch: 0157 loss_train: 0.2598 acc_train: 0.9786 loss_val: 0.6489 acc_val: 0.8000 time: 0.0166s\n",
            "Epoch: 0158 loss_train: 0.2851 acc_train: 0.9643 loss_val: 0.6536 acc_val: 0.8000 time: 0.0182s\n",
            "Epoch: 0159 loss_train: 0.2695 acc_train: 0.9714 loss_val: 0.6564 acc_val: 0.8000 time: 0.0167s\n",
            "Epoch: 0160 loss_train: 0.2981 acc_train: 0.9714 loss_val: 0.6552 acc_val: 0.8000 time: 0.0177s\n",
            "Epoch: 0161 loss_train: 0.3189 acc_train: 0.9714 loss_val: 0.6528 acc_val: 0.7967 time: 0.0188s\n",
            "Epoch: 0162 loss_train: 0.2482 acc_train: 0.9643 loss_val: 0.6499 acc_val: 0.8067 time: 0.0188s\n",
            "Epoch: 0163 loss_train: 0.3058 acc_train: 0.9429 loss_val: 0.6501 acc_val: 0.8033 time: 0.0183s\n",
            "Epoch: 0164 loss_train: 0.2740 acc_train: 0.9643 loss_val: 0.6508 acc_val: 0.8000 time: 0.0224s\n",
            "Epoch: 0165 loss_train: 0.2818 acc_train: 0.9714 loss_val: 0.6496 acc_val: 0.8000 time: 0.0207s\n",
            "Epoch: 0166 loss_train: 0.2449 acc_train: 0.9786 loss_val: 0.6487 acc_val: 0.7933 time: 0.0182s\n",
            "Epoch: 0167 loss_train: 0.2493 acc_train: 0.9571 loss_val: 0.6481 acc_val: 0.8000 time: 0.0173s\n",
            "Epoch: 0168 loss_train: 0.2954 acc_train: 0.9571 loss_val: 0.6468 acc_val: 0.7967 time: 0.0175s\n",
            "Epoch: 0169 loss_train: 0.2427 acc_train: 0.9857 loss_val: 0.6456 acc_val: 0.7967 time: 0.0171s\n",
            "Epoch: 0170 loss_train: 0.3039 acc_train: 0.9571 loss_val: 0.6445 acc_val: 0.7967 time: 0.0172s\n",
            "Epoch: 0171 loss_train: 0.2621 acc_train: 0.9643 loss_val: 0.6429 acc_val: 0.8000 time: 0.0195s\n",
            "Epoch: 0172 loss_train: 0.2847 acc_train: 0.9643 loss_val: 0.6424 acc_val: 0.8000 time: 0.0163s\n",
            "Epoch: 0173 loss_train: 0.2847 acc_train: 0.9571 loss_val: 0.6422 acc_val: 0.7967 time: 0.0198s\n",
            "Epoch: 0174 loss_train: 0.2748 acc_train: 0.9714 loss_val: 0.6445 acc_val: 0.7967 time: 0.0165s\n",
            "Epoch: 0175 loss_train: 0.2921 acc_train: 0.9429 loss_val: 0.6451 acc_val: 0.7933 time: 0.0200s\n",
            "Epoch: 0176 loss_train: 0.2542 acc_train: 0.9714 loss_val: 0.6458 acc_val: 0.7933 time: 0.0210s\n",
            "Epoch: 0177 loss_train: 0.2369 acc_train: 0.9857 loss_val: 0.6462 acc_val: 0.7900 time: 0.0194s\n",
            "Epoch: 0178 loss_train: 0.2735 acc_train: 0.9429 loss_val: 0.6476 acc_val: 0.7933 time: 0.0172s\n",
            "Epoch: 0179 loss_train: 0.2684 acc_train: 0.9714 loss_val: 0.6498 acc_val: 0.7933 time: 0.0173s\n",
            "Epoch: 0180 loss_train: 0.2653 acc_train: 0.9857 loss_val: 0.6513 acc_val: 0.7933 time: 0.0178s\n",
            "Epoch: 0181 loss_train: 0.3051 acc_train: 0.9714 loss_val: 0.6525 acc_val: 0.7933 time: 0.0183s\n",
            "Epoch: 0182 loss_train: 0.2759 acc_train: 0.9714 loss_val: 0.6510 acc_val: 0.7933 time: 0.0184s\n",
            "Epoch: 0183 loss_train: 0.2770 acc_train: 0.9786 loss_val: 0.6469 acc_val: 0.7900 time: 0.0177s\n",
            "Epoch: 0184 loss_train: 0.2668 acc_train: 0.9714 loss_val: 0.6417 acc_val: 0.7933 time: 0.0172s\n",
            "Epoch: 0185 loss_train: 0.3040 acc_train: 0.9714 loss_val: 0.6377 acc_val: 0.8033 time: 0.0164s\n",
            "Epoch: 0186 loss_train: 0.2709 acc_train: 0.9571 loss_val: 0.6358 acc_val: 0.8067 time: 0.0206s\n",
            "Epoch: 0187 loss_train: 0.2799 acc_train: 0.9643 loss_val: 0.6359 acc_val: 0.8067 time: 0.0180s\n",
            "Epoch: 0188 loss_train: 0.2743 acc_train: 0.9500 loss_val: 0.6371 acc_val: 0.8067 time: 0.0173s\n",
            "Epoch: 0189 loss_train: 0.2537 acc_train: 0.9714 loss_val: 0.6397 acc_val: 0.8067 time: 0.0188s\n",
            "Epoch: 0190 loss_train: 0.2562 acc_train: 0.9786 loss_val: 0.6430 acc_val: 0.8000 time: 0.0170s\n",
            "Epoch: 0191 loss_train: 0.2968 acc_train: 0.9643 loss_val: 0.6447 acc_val: 0.7933 time: 0.0193s\n",
            "Epoch: 0192 loss_train: 0.2859 acc_train: 0.9786 loss_val: 0.6444 acc_val: 0.7900 time: 0.0210s\n",
            "Epoch: 0193 loss_train: 0.2710 acc_train: 0.9500 loss_val: 0.6417 acc_val: 0.7900 time: 0.0187s\n",
            "Epoch: 0194 loss_train: 0.2631 acc_train: 0.9857 loss_val: 0.6399 acc_val: 0.7900 time: 0.0183s\n",
            "Epoch: 0195 loss_train: 0.2794 acc_train: 0.9643 loss_val: 0.6391 acc_val: 0.7933 time: 0.0179s\n",
            "Epoch: 0196 loss_train: 0.2519 acc_train: 0.9786 loss_val: 0.6371 acc_val: 0.7967 time: 0.0181s\n",
            "Epoch: 0197 loss_train: 0.2578 acc_train: 0.9857 loss_val: 0.6351 acc_val: 0.8000 time: 0.0183s\n",
            "Epoch: 0198 loss_train: 0.2425 acc_train: 0.9786 loss_val: 0.6330 acc_val: 0.8000 time: 0.0213s\n",
            "Epoch: 0199 loss_train: 0.2496 acc_train: 0.9714 loss_val: 0.6331 acc_val: 0.8000 time: 0.0191s\n",
            "Epoch: 0200 loss_train: 0.2709 acc_train: 0.9429 loss_val: 0.6337 acc_val: 0.8000 time: 0.0201s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 4.1229s\n",
            "Test set results: loss= 0.6390 accuracy= 0.8280\n"
          ]
        }
      ]
    }
  ]
}