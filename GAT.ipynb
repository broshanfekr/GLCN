{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMkEZszAt5o9NMxc8uolyB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/broshanfekr/GNN-models/blob/main/GAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import scipy.sparse as sp"
      ],
      "metadata": {
        "id": "lpgpyZ-FvWSD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utils**"
      ],
      "metadata": {
        "id": "-TllinzlwZ2h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "11cEeBr3upWN"
      },
      "outputs": [],
      "source": [
        "def encode_onehot(labels):\n",
        "    # The classes must be sorted before encoding to enable static class encoding.\n",
        "    # In other words, make sure the first class always maps to index 0.\n",
        "    classes = sorted(list(set(labels)))\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def load_data(path=\"./cora/\", dataset=\"cora\"):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize_features(features)\n",
        "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "\n",
        "def normalize_features(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model layers**"
      ],
      "metadata": {
        "id": "Hq-ImimWwB9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
        "        e = self._prepare_attentional_mechanism_input(Wh)\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, Wh)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def _prepare_attentional_mechanism_input(self, Wh):\n",
        "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
        "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
        "        # broadcast add\n",
        "        e = Wh1 + Wh2.T\n",
        "        return self.leakyrelu(e)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "class SpecialSpmmFunction(torch.autograd.Function):\n",
        "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, indices, values, shape, b):\n",
        "        assert indices.requires_grad == False\n",
        "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
        "        ctx.save_for_backward(a, b)\n",
        "        ctx.N = shape[0]\n",
        "        return torch.matmul(a, b)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        a, b = ctx.saved_tensors\n",
        "        grad_values = grad_b = None\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_a_dense = grad_output.matmul(b.t())\n",
        "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
        "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
        "        if ctx.needs_input_grad[3]:\n",
        "            grad_b = a.t().matmul(grad_output)\n",
        "        return None, grad_values, None, grad_b\n",
        "\n",
        "\n",
        "class SpecialSpmm(nn.Module):\n",
        "    def forward(self, indices, values, shape, b):\n",
        "        return SpecialSpmmFunction.apply(indices, values, shape, b)\n",
        "\n",
        "    \n",
        "class SpGraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(SpGraphAttentionLayer, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
        "                \n",
        "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
        "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "        self.special_spmm = SpecialSpmm()\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
        "\n",
        "        N = input.size()[0]\n",
        "        edge = adj.nonzero().t()\n",
        "\n",
        "        h = torch.mm(input, self.W)\n",
        "        # h: N x out\n",
        "        assert not torch.isnan(h).any()\n",
        "\n",
        "        # Self-attention on the nodes - Shared attention mechanism\n",
        "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
        "        # edge: 2*D x E\n",
        "\n",
        "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
        "        assert not torch.isnan(edge_e).any()\n",
        "        # edge_e: E\n",
        "\n",
        "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
        "        # e_rowsum: N x 1\n",
        "\n",
        "        edge_e = self.dropout(edge_e)\n",
        "        # edge_e: E\n",
        "\n",
        "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
        "        assert not torch.isnan(h_prime).any()\n",
        "        # h_prime: N x out\n",
        "        \n",
        "        h_prime = h_prime.div(e_rowsum)\n",
        "        # h_prime: N x out\n",
        "        assert not torch.isnan(h_prime).any()\n",
        "\n",
        "        if self.concat:\n",
        "            # if this layer is not last layer,\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            # if this layer is last layer,\n",
        "            return h_prime\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
      ],
      "metadata": {
        "id": "cpUSFj5Av7cm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the model**"
      ],
      "metadata": {
        "id": "_ScPgBAgwlKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
        "        super(GAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.elu(self.out_att(x, adj))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class SpGAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
        "        \"\"\"Sparse version of GAT.\"\"\"\n",
        "        super(SpGAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attentions = [SpGraphAttentionLayer(nfeat, \n",
        "                                                 nhid, \n",
        "                                                 dropout=dropout, \n",
        "                                                 alpha=alpha, \n",
        "                                                 concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n",
        "                                             nclass, \n",
        "                                             dropout=dropout, \n",
        "                                             alpha=alpha, \n",
        "                                             concat=False)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.elu(self.out_att(x, adj))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "PkVBXlYpwHty"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data**"
      ],
      "metadata": {
        "id": "DXlR4IKzx8Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1406bw-_6zLyNxAuQgWM2kanYpBbl7oqS\n",
        "!mkdir ./cora\n",
        "!unzip cora.zip -d ./cora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxW3imaCyAGU",
        "outputId": "3f3f366c-519c-4004-b673-aef150e2b842"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1406bw-_6zLyNxAuQgWM2kanYpBbl7oqS\n",
            "To: /content/cora.zip\n",
            "100% 153k/153k [00:00<00:00, 78.1MB/s]\n",
            "Archive:  cora.zip\n",
            "  inflating: ./cora/cora.cites       \n",
            "  inflating: ./cora/cora.content     \n",
            "  inflating: ./cora/README           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Model**"
      ],
      "metadata": {
        "id": "NJvkMoOHxFwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
        "parser.add_argument('--sparse', action='store_true', default=False, help='GAT with sparse version or not.')\n",
        "parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=8, help='Number of hidden units.')\n",
        "parser.add_argument('--nb_heads', type=int, default=8, help='Number of head attentions.')\n",
        "parser.add_argument('--dropout', type=float, default=0.6, help='Dropout rate (1 - keep probability).')\n",
        "parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
        "parser.add_argument('--patience', type=int, default=100, help='Patience')\n",
        "\n",
        "args = parser.parse_args()\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "if args.sparse:\n",
        "    model = SpGAT(nfeat=features.shape[1], \n",
        "                nhid=args.hidden, \n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=args.dropout, \n",
        "                nheads=args.nb_heads, \n",
        "                alpha=args.alpha)\n",
        "else:\n",
        "    model = GAT(nfeat=features.shape[1], \n",
        "                nhid=args.hidden, \n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=args.dropout, \n",
        "                nheads=args.nb_heads, \n",
        "                alpha=args.alpha)\n",
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=args.lr, \n",
        "                       weight_decay=args.weight_decay)\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if not args.fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_val.data.item()\n",
        "\n",
        "\n",
        "def compute_test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.data.item()))\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = args.epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(args.epochs):\n",
        "    loss_values.append(train(epoch))\n",
        "\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == args.patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model\n",
        "print('Loading {}th epoch'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "novn6wDKxErG",
        "outputId": "7b3441df-aa47-4ea8-aaa6-3d3c4100585c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9537 acc_train: 0.0714 loss_val: 1.9404 acc_val: 0.3400 time: 0.1976s\n",
            "Epoch: 0002 loss_train: 1.9420 acc_train: 0.2429 loss_val: 1.9309 acc_val: 0.5133 time: 0.1822s\n",
            "Epoch: 0003 loss_train: 1.9242 acc_train: 0.4286 loss_val: 1.9213 acc_val: 0.5267 time: 0.1655s\n",
            "Epoch: 0004 loss_train: 1.9183 acc_train: 0.3571 loss_val: 1.9116 acc_val: 0.5533 time: 0.1585s\n",
            "Epoch: 0005 loss_train: 1.9096 acc_train: 0.4214 loss_val: 1.9019 acc_val: 0.5700 time: 0.1527s\n",
            "Epoch: 0006 loss_train: 1.8992 acc_train: 0.4714 loss_val: 1.8922 acc_val: 0.5633 time: 0.1522s\n",
            "Epoch: 0007 loss_train: 1.8897 acc_train: 0.4357 loss_val: 1.8825 acc_val: 0.5700 time: 0.1485s\n",
            "Epoch: 0008 loss_train: 1.8761 acc_train: 0.5000 loss_val: 1.8727 acc_val: 0.5800 time: 0.1517s\n",
            "Epoch: 0009 loss_train: 1.8578 acc_train: 0.4786 loss_val: 1.8627 acc_val: 0.5800 time: 0.1517s\n",
            "Epoch: 0010 loss_train: 1.8522 acc_train: 0.4571 loss_val: 1.8526 acc_val: 0.5867 time: 0.1516s\n",
            "Epoch: 0011 loss_train: 1.8401 acc_train: 0.4786 loss_val: 1.8425 acc_val: 0.5867 time: 0.1477s\n",
            "Epoch: 0012 loss_train: 1.8136 acc_train: 0.5500 loss_val: 1.8320 acc_val: 0.5900 time: 0.1523s\n",
            "Epoch: 0013 loss_train: 1.8257 acc_train: 0.5786 loss_val: 1.8214 acc_val: 0.5900 time: 0.1517s\n",
            "Epoch: 0014 loss_train: 1.7832 acc_train: 0.5571 loss_val: 1.8104 acc_val: 0.5900 time: 0.1504s\n",
            "Epoch: 0015 loss_train: 1.7713 acc_train: 0.5643 loss_val: 1.7993 acc_val: 0.5900 time: 0.1487s\n",
            "Epoch: 0016 loss_train: 1.7824 acc_train: 0.5286 loss_val: 1.7881 acc_val: 0.5900 time: 0.1517s\n",
            "Epoch: 0017 loss_train: 1.7562 acc_train: 0.5500 loss_val: 1.7766 acc_val: 0.5967 time: 0.1528s\n",
            "Epoch: 0018 loss_train: 1.7375 acc_train: 0.6071 loss_val: 1.7649 acc_val: 0.5967 time: 0.1508s\n",
            "Epoch: 0019 loss_train: 1.7394 acc_train: 0.5071 loss_val: 1.7531 acc_val: 0.6033 time: 0.1479s\n",
            "Epoch: 0020 loss_train: 1.7099 acc_train: 0.5071 loss_val: 1.7411 acc_val: 0.6067 time: 0.1527s\n",
            "Epoch: 0021 loss_train: 1.6938 acc_train: 0.5286 loss_val: 1.7291 acc_val: 0.6067 time: 0.1517s\n",
            "Epoch: 0022 loss_train: 1.6552 acc_train: 0.5929 loss_val: 1.7167 acc_val: 0.6067 time: 0.1531s\n",
            "Epoch: 0023 loss_train: 1.6474 acc_train: 0.5857 loss_val: 1.7042 acc_val: 0.6067 time: 0.1480s\n",
            "Epoch: 0024 loss_train: 1.6673 acc_train: 0.5929 loss_val: 1.6917 acc_val: 0.6100 time: 0.1514s\n",
            "Epoch: 0025 loss_train: 1.6397 acc_train: 0.5571 loss_val: 1.6788 acc_val: 0.6067 time: 0.1511s\n",
            "Epoch: 0026 loss_train: 1.6056 acc_train: 0.6071 loss_val: 1.6658 acc_val: 0.6067 time: 0.1506s\n",
            "Epoch: 0027 loss_train: 1.6127 acc_train: 0.5857 loss_val: 1.6528 acc_val: 0.6133 time: 0.1475s\n",
            "Epoch: 0028 loss_train: 1.6092 acc_train: 0.5857 loss_val: 1.6399 acc_val: 0.6133 time: 0.1520s\n",
            "Epoch: 0029 loss_train: 1.6199 acc_train: 0.5643 loss_val: 1.6270 acc_val: 0.6167 time: 0.1520s\n",
            "Epoch: 0030 loss_train: 1.5802 acc_train: 0.5857 loss_val: 1.6142 acc_val: 0.6267 time: 0.1514s\n",
            "Epoch: 0031 loss_train: 1.5648 acc_train: 0.6143 loss_val: 1.6012 acc_val: 0.6267 time: 0.1482s\n",
            "Epoch: 0032 loss_train: 1.5754 acc_train: 0.6000 loss_val: 1.5884 acc_val: 0.6267 time: 0.1514s\n",
            "Epoch: 0033 loss_train: 1.5576 acc_train: 0.5857 loss_val: 1.5757 acc_val: 0.6333 time: 0.1508s\n",
            "Epoch: 0034 loss_train: 1.4717 acc_train: 0.6000 loss_val: 1.5629 acc_val: 0.6367 time: 0.1528s\n",
            "Epoch: 0035 loss_train: 1.4943 acc_train: 0.5714 loss_val: 1.5502 acc_val: 0.6367 time: 0.1486s\n",
            "Epoch: 0036 loss_train: 1.5268 acc_train: 0.5643 loss_val: 1.5376 acc_val: 0.6400 time: 0.1521s\n",
            "Epoch: 0037 loss_train: 1.4743 acc_train: 0.6000 loss_val: 1.5251 acc_val: 0.6400 time: 0.1523s\n",
            "Epoch: 0038 loss_train: 1.4617 acc_train: 0.5643 loss_val: 1.5130 acc_val: 0.6500 time: 0.1517s\n",
            "Epoch: 0039 loss_train: 1.3938 acc_train: 0.6643 loss_val: 1.5008 acc_val: 0.6567 time: 0.1482s\n",
            "Epoch: 0040 loss_train: 1.3825 acc_train: 0.6286 loss_val: 1.4885 acc_val: 0.6567 time: 0.1508s\n",
            "Epoch: 0041 loss_train: 1.4609 acc_train: 0.6143 loss_val: 1.4763 acc_val: 0.6600 time: 0.1521s\n",
            "Epoch: 0042 loss_train: 1.4123 acc_train: 0.6286 loss_val: 1.4642 acc_val: 0.6667 time: 0.1532s\n",
            "Epoch: 0043 loss_train: 1.4472 acc_train: 0.6286 loss_val: 1.4524 acc_val: 0.6733 time: 0.1471s\n",
            "Epoch: 0044 loss_train: 1.4309 acc_train: 0.6429 loss_val: 1.4411 acc_val: 0.6800 time: 0.1528s\n",
            "Epoch: 0045 loss_train: 1.4020 acc_train: 0.6143 loss_val: 1.4301 acc_val: 0.6833 time: 0.1516s\n",
            "Epoch: 0046 loss_train: 1.3687 acc_train: 0.6786 loss_val: 1.4193 acc_val: 0.6933 time: 0.1523s\n",
            "Epoch: 0047 loss_train: 1.3343 acc_train: 0.6214 loss_val: 1.4085 acc_val: 0.7067 time: 0.1481s\n",
            "Epoch: 0048 loss_train: 1.3257 acc_train: 0.6429 loss_val: 1.3978 acc_val: 0.7167 time: 0.1521s\n",
            "Epoch: 0049 loss_train: 1.3170 acc_train: 0.6643 loss_val: 1.3869 acc_val: 0.7133 time: 0.1522s\n",
            "Epoch: 0050 loss_train: 1.3183 acc_train: 0.6857 loss_val: 1.3762 acc_val: 0.7167 time: 0.1529s\n",
            "Epoch: 0051 loss_train: 1.3904 acc_train: 0.6571 loss_val: 1.3658 acc_val: 0.7200 time: 0.1484s\n",
            "Epoch: 0052 loss_train: 1.2833 acc_train: 0.7000 loss_val: 1.3552 acc_val: 0.7233 time: 0.1520s\n",
            "Epoch: 0053 loss_train: 1.2329 acc_train: 0.7500 loss_val: 1.3445 acc_val: 0.7267 time: 0.1512s\n",
            "Epoch: 0054 loss_train: 1.2214 acc_train: 0.7429 loss_val: 1.3340 acc_val: 0.7300 time: 0.1526s\n",
            "Epoch: 0055 loss_train: 1.1901 acc_train: 0.7286 loss_val: 1.3234 acc_val: 0.7333 time: 0.1489s\n",
            "Epoch: 0056 loss_train: 1.2463 acc_train: 0.6857 loss_val: 1.3133 acc_val: 0.7433 time: 0.1518s\n",
            "Epoch: 0057 loss_train: 1.2187 acc_train: 0.7643 loss_val: 1.3032 acc_val: 0.7567 time: 0.1520s\n",
            "Epoch: 0058 loss_train: 1.1834 acc_train: 0.7143 loss_val: 1.2931 acc_val: 0.7633 time: 0.1525s\n",
            "Epoch: 0059 loss_train: 1.2574 acc_train: 0.7429 loss_val: 1.2830 acc_val: 0.7733 time: 0.1484s\n",
            "Epoch: 0060 loss_train: 1.1831 acc_train: 0.7714 loss_val: 1.2731 acc_val: 0.7733 time: 0.1532s\n",
            "Epoch: 0061 loss_train: 1.2099 acc_train: 0.7214 loss_val: 1.2634 acc_val: 0.7800 time: 0.1518s\n",
            "Epoch: 0062 loss_train: 1.1375 acc_train: 0.7643 loss_val: 1.2534 acc_val: 0.7900 time: 0.1525s\n",
            "Epoch: 0063 loss_train: 1.2394 acc_train: 0.6786 loss_val: 1.2438 acc_val: 0.7933 time: 0.1476s\n",
            "Epoch: 0064 loss_train: 1.1322 acc_train: 0.7500 loss_val: 1.2344 acc_val: 0.7933 time: 0.1525s\n",
            "Epoch: 0065 loss_train: 1.1644 acc_train: 0.7286 loss_val: 1.2250 acc_val: 0.8000 time: 0.1524s\n",
            "Epoch: 0066 loss_train: 1.1269 acc_train: 0.7714 loss_val: 1.2157 acc_val: 0.8067 time: 0.1518s\n",
            "Epoch: 0067 loss_train: 1.1169 acc_train: 0.8286 loss_val: 1.2063 acc_val: 0.8100 time: 0.1494s\n",
            "Epoch: 0068 loss_train: 1.1416 acc_train: 0.7500 loss_val: 1.1973 acc_val: 0.8100 time: 0.1518s\n",
            "Epoch: 0069 loss_train: 1.0979 acc_train: 0.7857 loss_val: 1.1884 acc_val: 0.8133 time: 0.1522s\n",
            "Epoch: 0070 loss_train: 1.1539 acc_train: 0.7929 loss_val: 1.1795 acc_val: 0.8100 time: 0.1506s\n",
            "Epoch: 0071 loss_train: 1.0347 acc_train: 0.7500 loss_val: 1.1707 acc_val: 0.8100 time: 0.1482s\n",
            "Epoch: 0072 loss_train: 1.1624 acc_train: 0.7357 loss_val: 1.1621 acc_val: 0.8133 time: 0.1522s\n",
            "Epoch: 0073 loss_train: 1.1962 acc_train: 0.6857 loss_val: 1.1539 acc_val: 0.8100 time: 0.1514s\n",
            "Epoch: 0074 loss_train: 1.0774 acc_train: 0.7786 loss_val: 1.1458 acc_val: 0.8133 time: 0.1516s\n",
            "Epoch: 0075 loss_train: 0.9635 acc_train: 0.7714 loss_val: 1.1377 acc_val: 0.8133 time: 0.1496s\n",
            "Epoch: 0076 loss_train: 0.9923 acc_train: 0.7714 loss_val: 1.1298 acc_val: 0.8133 time: 0.1516s\n",
            "Epoch: 0077 loss_train: 1.0254 acc_train: 0.7571 loss_val: 1.1220 acc_val: 0.8100 time: 0.1517s\n",
            "Epoch: 0078 loss_train: 1.0315 acc_train: 0.7500 loss_val: 1.1146 acc_val: 0.8100 time: 0.1516s\n",
            "Epoch: 0079 loss_train: 1.0151 acc_train: 0.7714 loss_val: 1.1072 acc_val: 0.8100 time: 0.1481s\n",
            "Epoch: 0080 loss_train: 0.9866 acc_train: 0.7571 loss_val: 1.0998 acc_val: 0.8100 time: 0.1530s\n",
            "Epoch: 0081 loss_train: 0.9846 acc_train: 0.7857 loss_val: 1.0925 acc_val: 0.8100 time: 0.1522s\n",
            "Epoch: 0082 loss_train: 1.0107 acc_train: 0.7714 loss_val: 1.0856 acc_val: 0.8167 time: 0.1524s\n",
            "Epoch: 0083 loss_train: 1.0341 acc_train: 0.7357 loss_val: 1.0791 acc_val: 0.8167 time: 0.1481s\n",
            "Epoch: 0084 loss_train: 1.0133 acc_train: 0.7929 loss_val: 1.0728 acc_val: 0.8167 time: 0.1530s\n",
            "Epoch: 0085 loss_train: 0.9439 acc_train: 0.8000 loss_val: 1.0663 acc_val: 0.8167 time: 0.1524s\n",
            "Epoch: 0086 loss_train: 1.0490 acc_train: 0.7214 loss_val: 1.0598 acc_val: 0.8133 time: 0.1522s\n",
            "Epoch: 0087 loss_train: 1.0231 acc_train: 0.7429 loss_val: 1.0536 acc_val: 0.8167 time: 0.1476s\n",
            "Epoch: 0088 loss_train: 0.9191 acc_train: 0.8143 loss_val: 1.0477 acc_val: 0.8233 time: 0.1529s\n",
            "Epoch: 0089 loss_train: 0.9145 acc_train: 0.8000 loss_val: 1.0421 acc_val: 0.8233 time: 0.1521s\n",
            "Epoch: 0090 loss_train: 1.0128 acc_train: 0.7500 loss_val: 1.0366 acc_val: 0.8233 time: 0.1518s\n",
            "Epoch: 0091 loss_train: 0.9730 acc_train: 0.7714 loss_val: 1.0315 acc_val: 0.8233 time: 0.1478s\n",
            "Epoch: 0092 loss_train: 1.0097 acc_train: 0.7500 loss_val: 1.0263 acc_val: 0.8233 time: 0.1499s\n",
            "Epoch: 0093 loss_train: 0.9800 acc_train: 0.7571 loss_val: 1.0212 acc_val: 0.8233 time: 0.1517s\n",
            "Epoch: 0094 loss_train: 0.9835 acc_train: 0.7857 loss_val: 1.0162 acc_val: 0.8233 time: 0.1530s\n",
            "Epoch: 0095 loss_train: 1.0263 acc_train: 0.7286 loss_val: 1.0116 acc_val: 0.8200 time: 0.1486s\n",
            "Epoch: 0096 loss_train: 0.8781 acc_train: 0.7857 loss_val: 1.0069 acc_val: 0.8233 time: 0.1517s\n",
            "Epoch: 0097 loss_train: 1.0293 acc_train: 0.7357 loss_val: 1.0022 acc_val: 0.8200 time: 0.1506s\n",
            "Epoch: 0098 loss_train: 0.9546 acc_train: 0.7643 loss_val: 0.9977 acc_val: 0.8167 time: 0.1514s\n",
            "Epoch: 0099 loss_train: 0.9544 acc_train: 0.7714 loss_val: 0.9932 acc_val: 0.8233 time: 0.1483s\n",
            "Epoch: 0100 loss_train: 0.9179 acc_train: 0.7429 loss_val: 0.9889 acc_val: 0.8267 time: 0.1521s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.0300s\n",
            "Loading 99th epoch\n",
            "Test set results: loss= 1.0725 accuracy= 0.8190\n"
          ]
        }
      ]
    }
  ]
}