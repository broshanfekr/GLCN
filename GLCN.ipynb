{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GLCN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFiUDn9Fw1/Uvxxh6LC5Ua",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/broshanfekr/GLCN/blob/main/GLCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riqWWRETU610"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkLicwQWs1d"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dosUA2CWsTu"
      },
      "source": [
        "!mkdir -p data/cora/\n",
        "!wget -O data.zip https://www.dropbox.com/s/4lwcel37kpihzh8/cora.zip?dl=0\n",
        "!unzip data.zip -d /content/data/cora"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYeB5PVShtbd"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import sys\n",
        "import scipy.io as sio\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB-8pmFE3_OW"
      },
      "source": [
        "**مقدار دهی برخی از پارامترهای مربوط به آموزش مدل**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7AWAiN44QGK"
      },
      "source": [
        "class Flags:\n",
        "  def __init__(self):\n",
        "    self.dataset = 'cora'\n",
        "    self.model = 'sglcn'\n",
        "    self.lr1 = 0.005\n",
        "    self.lr2 = 0.005\n",
        "    self.epochs = 10000\n",
        "    self.hidden_gcn = 30\n",
        "    self.hidden_gl = 70\n",
        "    self.dropout = 0.6\n",
        "    self.weight_decay = 1e-4\n",
        "    self.early_stopping = 100\n",
        "    self.losslr1 = 0.01\n",
        "    self.losslr2 = 0.0001\n",
        "    self.seed = 123\n",
        "\n",
        "FLAGS = Flags()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg8BGcQw4WDF"
      },
      "source": [
        "**آماده سازی دیتاست جهت آموزش مدل**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsANP16cjUTU"
      },
      "source": [
        "def sample_mask(idx, l):\n",
        "    \"\"\"Create mask.\"\"\"\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "\n",
        "def load_data(dataset_str, path=\"data/\"):\n",
        "    path = path + dataset_str + \"/\"\n",
        "    if dataset_str == \"cora\":\n",
        "        features = sio.loadmat(path + \"feature\")\n",
        "        features = features['matrix']\n",
        "        adj = sio.loadmat(path + \"adj\")\n",
        "        adj = adj['matrix']\n",
        "        labels = sio.loadmat(path + \"label\")\n",
        "        labels = labels['matrix']\n",
        "        idx_train = range(140)\n",
        "        idx_val = range(200, 500)\n",
        "        idx_test = range(500, 1500)\n",
        "    elif dataset_str == \"citeseer\":\n",
        "        features = sio.loadmat(path + \"feature\")\n",
        "        features = features['matrix']\n",
        "        adj = sio.loadmat(path + \"adj\")\n",
        "        adj = adj['matrix']\n",
        "        labels = sio.loadmat(path + \"label\")\n",
        "        labels = labels['matrix']\n",
        "        idx_test = sio.loadmat(path + \"test.mat\")\n",
        "        idx_test = idx_test['array'].flatten()\n",
        "        idx_train = range(120)\n",
        "        idx_val = range(120, 620)\n",
        "    else:\n",
        "        features = sio.loadmat(path + \"feature\")\n",
        "        features = features['matrix']\n",
        "        adj = sio.loadmat(path + \"adj\")\n",
        "        adj = adj['matrix']\n",
        "        labels = sio.loadmat(path + \"label\")\n",
        "        labels = labels['matrix']\n",
        "        idx_test = sio.loadmat(path + \"test.mat\")\n",
        "        idx_test = idx_test['matrix']\n",
        "        idx_train = range(60)\n",
        "        idx_val = range(200, 500)\n",
        "\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
        "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
        "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
        "\n",
        "    y_train = np.zeros(labels.shape)\n",
        "    y_val = np.zeros(labels.shape)\n",
        "    y_test = np.zeros(labels.shape)\n",
        "    y_train[train_mask, :] = labels[train_mask, :]\n",
        "    y_val[val_mask, :] = labels[val_mask, :]\n",
        "    y_test[test_mask, :] = labels[test_mask, :]\n",
        "\n",
        "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQhsxRFV6_EP"
      },
      "source": [
        "**پیش پردازش ویژگی ها و همچنین پیش پردازش و نرمال سازی ماتریس مجاورت**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzXH4ip6694m"
      },
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "\n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return sparse_to_tuple(features)\n",
        "\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    edge = np.array(np.nonzero(adj_normalized.todense()))\n",
        "    return sparse_to_tuple(adj_normalized), edge"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQom9cb48b2d"
      },
      "source": [
        "**مقداردهی اولیه به وزن های شبکه عصبی و همچنین ایجاد ساختمان داده مربوط به ورودی شبکه عصبی**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f514Zez8cLH"
      },
      "source": [
        "def glorot(shape, name=None):\n",
        "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
        "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
        "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "    \n",
        "\n",
        "def construct_feed_dict(features, adj, labels, labels_mask, epoch, placeholders):\n",
        "    \"\"\"Construct feed dictionary.\"\"\"\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['labels']: labels})\n",
        "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['adj']: adj})\n",
        "    feed_dict.update({placeholders['step']: epoch})\n",
        "    feed_dict.update({placeholders['num_nodes']: features[2][0]})\n",
        "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
        "    return feed_dict"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCsNFjaJpXdG"
      },
      "source": [
        "**محاسبه معیارهای ارزیابی**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_KpHv59pY-T"
      },
      "source": [
        "def masked_softmax_cross_entropy(preds, labels, mask):\n",
        "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    loss *= mask\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "\n",
        "def masked_accuracy(preds, labels, mask):\n",
        "    \"\"\"Accuracy with masking.\"\"\"\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
        "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    accuracy_all *= mask\n",
        "    return tf.reduce_mean(accuracy_all)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4xuAsYwnST6"
      },
      "source": [
        "**تعریف لایه کانولوشن گرافی و همچنین لایه مربوط به یادگیری ساختار گراف**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPl-9DlEqmiI"
      },
      "source": [
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "\n",
        "def sparse_dropout(x, keep_prob, noise_shape):\n",
        "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "\n",
        "def dot(x, y, sparse=False):\n",
        "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
        "    if sparse:\n",
        "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "        res = tf.matmul(x, y)\n",
        "    return res\n",
        "\n",
        "\n",
        "class SparseGraphLearn(object):\n",
        "    \"\"\"Sparse Graph learning layer.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, edge, placeholders, dropout=0.,\n",
        "                 sparse_inputs=False, act=tf.nn.relu, bias=False, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "\n",
        "        if dropout:\n",
        "            self.dropout = placeholders['dropout']\n",
        "        else:\n",
        "            self.dropout = 0.\n",
        "\n",
        "        self.act = act\n",
        "        self.num_nodes = placeholders['num_nodes']\n",
        "        self.sparse_inputs = sparse_inputs\n",
        "        self.bias = bias\n",
        "        self.edge = edge\n",
        "\n",
        "        # helper variable for sparse dropout\n",
        "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
        "\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            self.vars['weights'] = glorot([input_dim, output_dim], name='weights')\n",
        "            self.vars['a'] = glorot([output_dim, 1], name='a')\n",
        "            if self.bias:\n",
        "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        # dropout\n",
        "        if self.sparse_inputs:\n",
        "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
        "        else:\n",
        "            x = tf.nn.dropout(x, 1-self.dropout)\n",
        "\n",
        "        # graph learning\n",
        "        h = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
        "        N = self.num_nodes\n",
        "        edge_v = tf.abs(tf.gather(h,self.edge[0]) - tf.gather(h,self.edge[1]))\n",
        "        edge_v = tf.squeeze(self.act(dot(edge_v, self.vars['a'])))\n",
        "        sgraph = tf.SparseTensor(indices=tf.transpose(self.edge), values=edge_v, dense_shape=[N, N])\n",
        "        sgraph = tf.sparse_softmax(sgraph)\n",
        "        return h, sgraph\n",
        "\n",
        "\n",
        "class GraphConvolution(object):\n",
        "    \"\"\"Graph convolution layer provided by Thomas N. Kipf, Max Welling, \n",
        "    [Semi-Supervised Classification with Graph Convolutional Networks]\n",
        "    (http://arxiv.org/abs/1609.02907) (ICLR 2017)\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
        "                 sparse_inputs=False, act=tf.nn.relu, bias=False, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        if dropout:\n",
        "            self.dropout = placeholders['dropout']\n",
        "        else:\n",
        "            self.dropout = 0.\n",
        "\n",
        "        self.act = act\n",
        "        self.sparse_inputs = sparse_inputs\n",
        "        self.bias = bias\n",
        "\n",
        "        # helper variable for sparse dropout\n",
        "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
        "\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            self.vars['weights'] = glorot([input_dim, output_dim], name='weights')\n",
        "            if self.bias:\n",
        "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
        "\n",
        "        if self.logging:\n",
        "            self._log_vars()\n",
        "\n",
        "    def _call(self, inputs, adj):\n",
        "        x = inputs\n",
        "\n",
        "        # dropout\n",
        "        if self.sparse_inputs:\n",
        "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
        "        else:\n",
        "            x = tf.nn.dropout(x, 1-self.dropout)\n",
        "\n",
        "        # convolve\n",
        "        pre_sup = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
        "        output = dot(adj, pre_sup, sparse=True)\n",
        "\n",
        "        # bias\n",
        "        if self.bias:\n",
        "            output += self.vars['bias']\n",
        "\n",
        "        return self.act(output)\n",
        "\n",
        "    def __call__(self, inputs, adj):\n",
        "        with tf.name_scope(self.name):\n",
        "            if self.logging and not self.sparse_inputs:\n",
        "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
        "            outputs = self._call(inputs, adj)\n",
        "            if self.logging:\n",
        "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
        "            return outputs\n",
        "\n",
        "    def _log_vars(self):\n",
        "        for var in self.vars:\n",
        "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUXpSUf5noDQ"
      },
      "source": [
        "**ساخت مدل نهایی**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vld34gtEq0Ed"
      },
      "source": [
        "class SGLCN(object):\n",
        "    def __init__(self, placeholders, edge, input_dim, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.loss1 = 0\n",
        "        self.loss2 = 0\n",
        "\n",
        "        self.inputs = placeholders['features']\n",
        "        self.edge = edge\n",
        "        self.input_dim = input_dim\n",
        "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
        "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
        "        self.placeholders = placeholders\n",
        "\n",
        "        learning_rate1 = tf.train.exponential_decay(learning_rate=FLAGS.lr1, global_step=placeholders['step'],\n",
        "                                                    decay_steps=100, decay_rate=0.9, staircase=True)\n",
        "        learning_rate2 = tf.train.exponential_decay(learning_rate=FLAGS.lr2, global_step=placeholders['step'],\n",
        "                                                    decay_steps=100, decay_rate=0.9, staircase=True)\n",
        "        self.optimizer1 = tf.train.AdamOptimizer(learning_rate=learning_rate1)\n",
        "        self.optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate2)\n",
        "\n",
        "        self.layers0 = SparseGraphLearn(input_dim=self.input_dim,\n",
        "                                        output_dim=FLAGS.hidden_gl,\n",
        "                                        edge=self.edge,\n",
        "                                        placeholders=self.placeholders,\n",
        "                                        act=tf.nn.relu,\n",
        "                                        dropout=True,\n",
        "                                        sparse_inputs=True)\n",
        "\n",
        "        self.layers1 = GraphConvolution(input_dim=self.input_dim,\n",
        "                                        output_dim=FLAGS.hidden_gcn,\n",
        "                                        placeholders=self.placeholders,\n",
        "                                        act=tf.nn.relu,\n",
        "                                        dropout=True,\n",
        "                                        sparse_inputs=True,\n",
        "                                        logging=self.logging)\n",
        "\n",
        "        self.layers2 = GraphConvolution(input_dim=FLAGS.hidden_gcn,\n",
        "                                        output_dim=self.output_dim,\n",
        "                                        placeholders=self.placeholders,\n",
        "                                        act=lambda x: x,\n",
        "                                        dropout=True,\n",
        "                                        logging=self.logging)\n",
        "        self.build()\n",
        "        self.pro = tf.nn.softmax(self.outputs)\n",
        "\n",
        "    def _loss(self):\n",
        "        # Weight decay loss\n",
        "        for var in self.layers0.vars.values():\n",
        "            self.loss1 += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
        "        for var in self.layers1.vars.values():\n",
        "            self.loss2 += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
        "\n",
        "        # Graph Learning loss\n",
        "        D = tf.matrix_diag(tf.ones(self.placeholders['num_nodes']))*-1\n",
        "        D = tf.sparse_add(D, self.S)*-1\n",
        "        D = tf.matmul(tf.transpose(self.x), D)\n",
        "        self.loss1 += tf.trace(tf.matmul(D, self.x)) * FLAGS.losslr1\n",
        "        self.loss1 -= tf.trace(tf.sparse_tensor_dense_matmul(tf.sparse_transpose(self.S), tf.sparse_tensor_to_dense(self.S))) * FLAGS.losslr2\n",
        "\n",
        "        # Cross entropy error\n",
        "        self.loss2 += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
        "                                                  self.placeholders['labels_mask'])\n",
        "       \n",
        "        self.loss = self.loss1 + self.loss2\n",
        "\n",
        "    def _accuracy(self):\n",
        "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
        "                                        self.placeholders['labels_mask'])\n",
        "\n",
        "    def build(self):\n",
        "        self.x, self.S = self.layers0(self.inputs)\n",
        "\n",
        "        x1 = self.layers1(self.inputs, self.S)\n",
        "        self.outputs = self.layers2(x1, self.S)\n",
        "\n",
        "        # Store model variables for easy access\n",
        "        self.vars1 = tf.trainable_variables()[0:2]\n",
        "        self.vars2 = tf.trainable_variables()[2:]\n",
        "\n",
        "        # Build metrics\n",
        "        self._loss()\n",
        "        self._accuracy()\n",
        "\n",
        "        self.opt_op1 = self.optimizer1.minimize(self.loss1, var_list=self.vars1)\n",
        "        self.opt_op2 = self.optimizer2.minimize(self.loss2, var_list=self.vars2)\n",
        "        self.opt_op = tf.group(self.opt_op1, self.opt_op2)\n",
        "\n",
        "    def predict(self):\n",
        "        return tf.nn.softmax(self.outputs)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpnxy2SAq1sB",
        "outputId": "940c46b6-2326-47fc-f57a-85a8e323d9bc"
      },
      "source": [
        "# Define model evaluation function\n",
        "def evaluate(features, adj, labels, mask, epoch, placeholders,flag=0):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val = construct_feed_dict(features, adj, labels, mask, epoch, placeholders)\n",
        "    if flag == 0:\n",
        "        outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
        "        return outs_val[0], outs_val[1], (time.time() - t_test)\n",
        "    else:\n",
        "        outs_val = sess.run(model.accuracy, feed_dict=feed_dict_val)\n",
        "        return outs_val\n",
        "\n",
        "np.random.seed(FLAGS.seed)\n",
        "tf.set_random_seed(FLAGS.seed)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
        "# Load data\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)\n",
        "\n",
        "# Some preprocessing\n",
        "features = preprocess_features(features)\n",
        "adj, edge = preprocess_adj(adj)\n",
        "\n",
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'adj': tf.sparse_placeholder(tf.float32),\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    'num_nodes': tf.placeholder(tf.int32),\n",
        "    'step': tf.placeholder(tf.int32),\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
        "}\n",
        "\n",
        "# Create model\n",
        "model = SGLCN(placeholders, edge, input_dim=features[2][1], logging=True)\n",
        "\n",
        "# Initialize session\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "config = tf.ConfigProto()  \n",
        "config.gpu_options.per_process_gpu_memory_fraction = 1\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "# sess = tf.Session()\n",
        "\n",
        "# Init variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "test_acc_list = []\n",
        "best_epoch = 0\n",
        "best = 10000\n",
        "\n",
        "# Train model\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    feed_dict = construct_feed_dict(features, adj, y_train, train_mask, epoch, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "    # Training step\n",
        "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
        "    # Validation\n",
        "    cost, acc, duration = evaluate(features, adj, y_val, val_mask, epoch, placeholders)\n",
        "    test_acc = evaluate(features, adj, y_test, test_mask, epoch, placeholders, flag=1)\n",
        "    test_acc_list.append(test_acc)\n",
        "\n",
        "\n",
        "    # Print results\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
        "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
        "          \"val_acc=\", \"{:.5f}\".format(acc), \"test_acc=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "    if cost < best:\n",
        "        best_epoch = epoch\n",
        "        best = cost\n",
        "        patience = 0\n",
        "\n",
        "    else:\n",
        "        patience += 1\n",
        "\n",
        "    if patience == FLAGS.early_stopping:\n",
        "        # feed_dict_val = construct_feed_dict(features, adj, y_test, test_mask, epoch, placeholders)\n",
        "        # Smap = sess.run(tf.sparse_tensor_to_dense(model.S), feed_dict=feed_dict_val)\n",
        "        # sio.savemat(\"S.mat\", {'adjfix': np.array(Smap)})\n",
        "        break\n",
        " \n",
        "print(\"Optimization Finished!\")\n",
        "print(\"----------------------------------------------\")\n",
        "print(\"The finall result:\", test_acc_list[-101])\n",
        "print(\"----------------------------------------------\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-27-0aa71d5f0941>:129: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-26-056bee6d3e3f>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch: 0001 train_loss= 2.19247 train_acc= 0.21429 val_loss= 1.96447 val_acc= 0.26000 test_acc= 0.30400 time= 1.05668\n",
            "Epoch: 0002 train_loss= 2.11923 train_acc= 0.40000 val_loss= 1.94037 val_acc= 0.46333 test_acc= 0.43100 time= 0.36667\n",
            "Epoch: 0003 train_loss= 2.06540 train_acc= 0.50714 val_loss= 1.91946 val_acc= 0.54333 test_acc= 0.47400 time= 0.36518\n",
            "Epoch: 0004 train_loss= 2.02314 train_acc= 0.55000 val_loss= 1.90130 val_acc= 0.58333 test_acc= 0.47800 time= 0.35680\n",
            "Epoch: 0005 train_loss= 1.98001 train_acc= 0.57143 val_loss= 1.88563 val_acc= 0.59000 test_acc= 0.48000 time= 0.36787\n",
            "Epoch: 0006 train_loss= 1.94517 train_acc= 0.60714 val_loss= 1.87210 val_acc= 0.58667 test_acc= 0.47500 time= 0.36023\n",
            "Epoch: 0007 train_loss= 1.91580 train_acc= 0.57857 val_loss= 1.86040 val_acc= 0.58333 test_acc= 0.47700 time= 0.36359\n",
            "Epoch: 0008 train_loss= 1.89027 train_acc= 0.60000 val_loss= 1.85021 val_acc= 0.58667 test_acc= 0.48000 time= 0.35567\n",
            "Epoch: 0009 train_loss= 1.86795 train_acc= 0.63571 val_loss= 1.84117 val_acc= 0.58667 test_acc= 0.47700 time= 0.35939\n",
            "Epoch: 0010 train_loss= 1.85384 train_acc= 0.63571 val_loss= 1.83296 val_acc= 0.59667 test_acc= 0.47500 time= 0.39736\n",
            "Epoch: 0011 train_loss= 1.83579 train_acc= 0.62143 val_loss= 1.82534 val_acc= 0.60000 test_acc= 0.47700 time= 0.36130\n",
            "Epoch: 0012 train_loss= 1.82680 train_acc= 0.62143 val_loss= 1.81803 val_acc= 0.60667 test_acc= 0.47700 time= 0.36789\n",
            "Epoch: 0013 train_loss= 1.81623 train_acc= 0.64286 val_loss= 1.81083 val_acc= 0.60667 test_acc= 0.47800 time= 0.36182\n",
            "Epoch: 0014 train_loss= 1.80470 train_acc= 0.61429 val_loss= 1.80364 val_acc= 0.60333 test_acc= 0.47800 time= 0.36339\n",
            "Epoch: 0015 train_loss= 1.78814 train_acc= 0.62857 val_loss= 1.79618 val_acc= 0.60000 test_acc= 0.47600 time= 0.35702\n",
            "Epoch: 0016 train_loss= 1.77915 train_acc= 0.65000 val_loss= 1.78840 val_acc= 0.60000 test_acc= 0.47000 time= 0.36743\n",
            "Epoch: 0017 train_loss= 1.77908 train_acc= 0.65714 val_loss= 1.78035 val_acc= 0.59667 test_acc= 0.46900 time= 0.36159\n",
            "Epoch: 0018 train_loss= 1.77425 train_acc= 0.61429 val_loss= 1.77194 val_acc= 0.59667 test_acc= 0.46900 time= 0.36240\n",
            "Epoch: 0019 train_loss= 1.74376 train_acc= 0.65000 val_loss= 1.76315 val_acc= 0.59333 test_acc= 0.46500 time= 0.36319\n",
            "Epoch: 0020 train_loss= 1.74524 train_acc= 0.62857 val_loss= 1.75408 val_acc= 0.59333 test_acc= 0.46400 time= 0.36083\n",
            "Epoch: 0021 train_loss= 1.73633 train_acc= 0.65000 val_loss= 1.74471 val_acc= 0.59333 test_acc= 0.46400 time= 0.36546\n",
            "Epoch: 0022 train_loss= 1.71656 train_acc= 0.62857 val_loss= 1.73505 val_acc= 0.59333 test_acc= 0.46300 time= 0.36427\n",
            "Epoch: 0023 train_loss= 1.70671 train_acc= 0.65714 val_loss= 1.72513 val_acc= 0.59333 test_acc= 0.46200 time= 0.34949\n",
            "Epoch: 0024 train_loss= 1.70125 train_acc= 0.57143 val_loss= 1.71502 val_acc= 0.59000 test_acc= 0.45900 time= 0.36505\n",
            "Epoch: 0025 train_loss= 1.67445 train_acc= 0.60714 val_loss= 1.70464 val_acc= 0.59000 test_acc= 0.45800 time= 0.35506\n",
            "Epoch: 0026 train_loss= 1.65230 train_acc= 0.62857 val_loss= 1.69408 val_acc= 0.58667 test_acc= 0.45700 time= 0.36160\n",
            "Epoch: 0027 train_loss= 1.66064 train_acc= 0.62143 val_loss= 1.68341 val_acc= 0.58667 test_acc= 0.45600 time= 0.37088\n",
            "Epoch: 0028 train_loss= 1.64154 train_acc= 0.62857 val_loss= 1.67259 val_acc= 0.58667 test_acc= 0.45600 time= 0.36688\n",
            "Epoch: 0029 train_loss= 1.62253 train_acc= 0.62857 val_loss= 1.66158 val_acc= 0.58667 test_acc= 0.45400 time= 0.35433\n",
            "Epoch: 0030 train_loss= 1.60219 train_acc= 0.60000 val_loss= 1.65040 val_acc= 0.58667 test_acc= 0.45300 time= 0.37287\n",
            "Epoch: 0031 train_loss= 1.59486 train_acc= 0.65714 val_loss= 1.63915 val_acc= 0.58333 test_acc= 0.45400 time= 0.35285\n",
            "Epoch: 0032 train_loss= 1.60481 train_acc= 0.62143 val_loss= 1.62785 val_acc= 0.58333 test_acc= 0.45200 time= 0.34956\n",
            "Epoch: 0033 train_loss= 1.55983 train_acc= 0.62857 val_loss= 1.61639 val_acc= 0.58333 test_acc= 0.45100 time= 0.36495\n",
            "Epoch: 0034 train_loss= 1.55866 train_acc= 0.62857 val_loss= 1.60483 val_acc= 0.58333 test_acc= 0.45100 time= 0.35849\n",
            "Epoch: 0035 train_loss= 1.54313 train_acc= 0.62857 val_loss= 1.59312 val_acc= 0.58333 test_acc= 0.45100 time= 0.35999\n",
            "Epoch: 0036 train_loss= 1.52845 train_acc= 0.67143 val_loss= 1.58129 val_acc= 0.58333 test_acc= 0.45300 time= 0.37143\n",
            "Epoch: 0037 train_loss= 1.49442 train_acc= 0.69286 val_loss= 1.56935 val_acc= 0.59000 test_acc= 0.45400 time= 0.36345\n",
            "Epoch: 0038 train_loss= 1.49322 train_acc= 0.66429 val_loss= 1.55734 val_acc= 0.59333 test_acc= 0.45200 time= 0.37521\n",
            "Epoch: 0039 train_loss= 1.47141 train_acc= 0.62857 val_loss= 1.54525 val_acc= 0.59333 test_acc= 0.45200 time= 0.36340\n",
            "Epoch: 0040 train_loss= 1.46853 train_acc= 0.62143 val_loss= 1.53308 val_acc= 0.59333 test_acc= 0.45200 time= 0.35552\n",
            "Epoch: 0041 train_loss= 1.43822 train_acc= 0.64286 val_loss= 1.52080 val_acc= 0.59333 test_acc= 0.45300 time= 0.36468\n",
            "Epoch: 0042 train_loss= 1.42365 train_acc= 0.65000 val_loss= 1.50847 val_acc= 0.59667 test_acc= 0.45400 time= 0.35306\n",
            "Epoch: 0043 train_loss= 1.41442 train_acc= 0.62143 val_loss= 1.49616 val_acc= 0.59667 test_acc= 0.45700 time= 0.35235\n",
            "Epoch: 0044 train_loss= 1.40979 train_acc= 0.61429 val_loss= 1.48385 val_acc= 0.59667 test_acc= 0.46100 time= 0.37244\n",
            "Epoch: 0045 train_loss= 1.40381 train_acc= 0.66429 val_loss= 1.47156 val_acc= 0.60333 test_acc= 0.46600 time= 0.36265\n",
            "Epoch: 0046 train_loss= 1.36971 train_acc= 0.65714 val_loss= 1.45923 val_acc= 0.60333 test_acc= 0.46900 time= 0.36789\n",
            "Epoch: 0047 train_loss= 1.34190 train_acc= 0.65714 val_loss= 1.44689 val_acc= 0.61333 test_acc= 0.47300 time= 0.37390\n",
            "Epoch: 0048 train_loss= 1.36010 train_acc= 0.65714 val_loss= 1.43458 val_acc= 0.62000 test_acc= 0.47700 time= 0.36505\n",
            "Epoch: 0049 train_loss= 1.34387 train_acc= 0.66429 val_loss= 1.42234 val_acc= 0.62000 test_acc= 0.48400 time= 0.36092\n",
            "Epoch: 0050 train_loss= 1.32391 train_acc= 0.67857 val_loss= 1.41011 val_acc= 0.62000 test_acc= 0.48800 time= 0.36627\n",
            "Epoch: 0051 train_loss= 1.28242 train_acc= 0.67857 val_loss= 1.39789 val_acc= 0.62000 test_acc= 0.49200 time= 0.35013\n",
            "Epoch: 0052 train_loss= 1.28243 train_acc= 0.66429 val_loss= 1.38573 val_acc= 0.62333 test_acc= 0.49800 time= 0.35489\n",
            "Epoch: 0053 train_loss= 1.25946 train_acc= 0.70714 val_loss= 1.37361 val_acc= 0.63333 test_acc= 0.50700 time= 0.37159\n",
            "Epoch: 0054 train_loss= 1.24055 train_acc= 0.72143 val_loss= 1.36152 val_acc= 0.63667 test_acc= 0.51600 time= 0.35383\n",
            "Epoch: 0055 train_loss= 1.23358 train_acc= 0.69286 val_loss= 1.34951 val_acc= 0.63667 test_acc= 0.52500 time= 0.36240\n",
            "Epoch: 0056 train_loss= 1.21211 train_acc= 0.72143 val_loss= 1.33757 val_acc= 0.63667 test_acc= 0.53300 time= 0.35717\n",
            "Epoch: 0057 train_loss= 1.23142 train_acc= 0.71429 val_loss= 1.32574 val_acc= 0.63667 test_acc= 0.54200 time= 0.35194\n",
            "Epoch: 0058 train_loss= 1.16655 train_acc= 0.71429 val_loss= 1.31399 val_acc= 0.64667 test_acc= 0.54600 time= 0.36850\n",
            "Epoch: 0059 train_loss= 1.19447 train_acc= 0.69286 val_loss= 1.30232 val_acc= 0.65333 test_acc= 0.55200 time= 0.36495\n",
            "Epoch: 0060 train_loss= 1.17275 train_acc= 0.72857 val_loss= 1.29071 val_acc= 0.66667 test_acc= 0.55600 time= 0.35881\n",
            "Epoch: 0061 train_loss= 1.13267 train_acc= 0.77143 val_loss= 1.27920 val_acc= 0.67333 test_acc= 0.56200 time= 0.36178\n",
            "Epoch: 0062 train_loss= 1.11574 train_acc= 0.75000 val_loss= 1.26777 val_acc= 0.68333 test_acc= 0.57500 time= 0.34756\n",
            "Epoch: 0063 train_loss= 1.08750 train_acc= 0.77143 val_loss= 1.25648 val_acc= 0.69000 test_acc= 0.58100 time= 0.35092\n",
            "Epoch: 0064 train_loss= 1.10985 train_acc= 0.75714 val_loss= 1.24535 val_acc= 0.69667 test_acc= 0.59100 time= 0.37341\n",
            "Epoch: 0065 train_loss= 1.11646 train_acc= 0.76429 val_loss= 1.23441 val_acc= 0.70333 test_acc= 0.60600 time= 0.35104\n",
            "Epoch: 0066 train_loss= 1.06905 train_acc= 0.77857 val_loss= 1.22358 val_acc= 0.71000 test_acc= 0.62000 time= 0.34912\n",
            "Epoch: 0067 train_loss= 1.09521 train_acc= 0.75000 val_loss= 1.21287 val_acc= 0.71333 test_acc= 0.63100 time= 0.37347\n",
            "Epoch: 0068 train_loss= 1.03989 train_acc= 0.80714 val_loss= 1.20222 val_acc= 0.72000 test_acc= 0.64100 time= 0.35576\n",
            "Epoch: 0069 train_loss= 1.03339 train_acc= 0.77857 val_loss= 1.19174 val_acc= 0.72667 test_acc= 0.65000 time= 0.35541\n",
            "Epoch: 0070 train_loss= 1.03008 train_acc= 0.83571 val_loss= 1.18137 val_acc= 0.73667 test_acc= 0.65400 time= 0.37277\n",
            "Epoch: 0071 train_loss= 1.03670 train_acc= 0.78571 val_loss= 1.17115 val_acc= 0.73667 test_acc= 0.66100 time= 0.35082\n",
            "Epoch: 0072 train_loss= 1.01012 train_acc= 0.81429 val_loss= 1.16111 val_acc= 0.75333 test_acc= 0.66500 time= 0.35180\n",
            "Epoch: 0073 train_loss= 1.01992 train_acc= 0.81429 val_loss= 1.15119 val_acc= 0.75667 test_acc= 0.67200 time= 0.35807\n",
            "Epoch: 0074 train_loss= 0.98377 train_acc= 0.82143 val_loss= 1.14138 val_acc= 0.75667 test_acc= 0.67900 time= 0.34690\n",
            "Epoch: 0075 train_loss= 0.96784 train_acc= 0.81429 val_loss= 1.13167 val_acc= 0.76667 test_acc= 0.68600 time= 0.35546\n",
            "Epoch: 0076 train_loss= 0.97901 train_acc= 0.80000 val_loss= 1.12206 val_acc= 0.76667 test_acc= 0.68700 time= 0.37072\n",
            "Epoch: 0077 train_loss= 0.92262 train_acc= 0.83571 val_loss= 1.11262 val_acc= 0.76667 test_acc= 0.69500 time= 0.34921\n",
            "Epoch: 0078 train_loss= 0.90439 train_acc= 0.86429 val_loss= 1.10331 val_acc= 0.77000 test_acc= 0.69800 time= 0.35870\n",
            "Epoch: 0079 train_loss= 0.93790 train_acc= 0.82857 val_loss= 1.09411 val_acc= 0.77333 test_acc= 0.69800 time= 0.35122\n",
            "Epoch: 0080 train_loss= 0.89999 train_acc= 0.81429 val_loss= 1.08495 val_acc= 0.77667 test_acc= 0.70300 time= 0.35851\n",
            "Epoch: 0081 train_loss= 0.87738 train_acc= 0.87143 val_loss= 1.07599 val_acc= 0.78000 test_acc= 0.70900 time= 0.36078\n",
            "Epoch: 0082 train_loss= 0.90557 train_acc= 0.82143 val_loss= 1.06712 val_acc= 0.78000 test_acc= 0.71600 time= 0.35281\n",
            "Epoch: 0083 train_loss= 0.89148 train_acc= 0.84286 val_loss= 1.05843 val_acc= 0.78000 test_acc= 0.72100 time= 0.35562\n",
            "Epoch: 0084 train_loss= 0.85854 train_acc= 0.84286 val_loss= 1.04980 val_acc= 0.78333 test_acc= 0.72400 time= 0.36011\n",
            "Epoch: 0085 train_loss= 0.80779 train_acc= 0.87857 val_loss= 1.04139 val_acc= 0.78333 test_acc= 0.73000 time= 0.35622\n",
            "Epoch: 0086 train_loss= 0.83865 train_acc= 0.85000 val_loss= 1.03313 val_acc= 0.78333 test_acc= 0.73600 time= 0.34567\n",
            "Epoch: 0087 train_loss= 0.81696 train_acc= 0.85714 val_loss= 1.02511 val_acc= 0.78667 test_acc= 0.74200 time= 0.36159\n",
            "Epoch: 0088 train_loss= 0.82396 train_acc= 0.87143 val_loss= 1.01724 val_acc= 0.78667 test_acc= 0.74200 time= 0.35045\n",
            "Epoch: 0089 train_loss= 0.81851 train_acc= 0.87857 val_loss= 1.00957 val_acc= 0.78667 test_acc= 0.74500 time= 0.36452\n",
            "Epoch: 0090 train_loss= 0.79866 train_acc= 0.85000 val_loss= 1.00198 val_acc= 0.79000 test_acc= 0.74700 time= 0.36431\n",
            "Epoch: 0091 train_loss= 0.82244 train_acc= 0.85714 val_loss= 0.99465 val_acc= 0.79000 test_acc= 0.75000 time= 0.34319\n",
            "Epoch: 0092 train_loss= 0.76753 train_acc= 0.88571 val_loss= 0.98739 val_acc= 0.79000 test_acc= 0.75100 time= 0.34729\n",
            "Epoch: 0093 train_loss= 0.75174 train_acc= 0.87857 val_loss= 0.98025 val_acc= 0.79000 test_acc= 0.75200 time= 0.36664\n",
            "Epoch: 0094 train_loss= 0.79573 train_acc= 0.85000 val_loss= 0.97320 val_acc= 0.79333 test_acc= 0.75200 time= 0.36311\n",
            "Epoch: 0095 train_loss= 0.73923 train_acc= 0.86429 val_loss= 0.96621 val_acc= 0.80000 test_acc= 0.75600 time= 0.36006\n",
            "Epoch: 0096 train_loss= 0.80485 train_acc= 0.90000 val_loss= 0.95942 val_acc= 0.80000 test_acc= 0.75900 time= 0.75397\n",
            "Epoch: 0097 train_loss= 0.75429 train_acc= 0.86429 val_loss= 0.95277 val_acc= 0.80000 test_acc= 0.76400 time= 0.36405\n",
            "Epoch: 0098 train_loss= 0.75610 train_acc= 0.86429 val_loss= 0.94623 val_acc= 0.80667 test_acc= 0.76800 time= 0.34748\n",
            "Epoch: 0099 train_loss= 0.74871 train_acc= 0.86429 val_loss= 0.93979 val_acc= 0.80667 test_acc= 0.77100 time= 0.34966\n",
            "Epoch: 0100 train_loss= 0.72431 train_acc= 0.89286 val_loss= 0.93340 val_acc= 0.80667 test_acc= 0.77200 time= 0.36807\n",
            "Epoch: 0101 train_loss= 0.71569 train_acc= 0.85714 val_loss= 0.92777 val_acc= 0.80667 test_acc= 0.77400 time= 0.35246\n",
            "Epoch: 0102 train_loss= 0.69974 train_acc= 0.92857 val_loss= 0.92221 val_acc= 0.80667 test_acc= 0.77700 time= 0.36315\n",
            "Epoch: 0103 train_loss= 0.66675 train_acc= 0.90000 val_loss= 0.91687 val_acc= 0.81000 test_acc= 0.77900 time= 0.36948\n",
            "Epoch: 0104 train_loss= 0.74326 train_acc= 0.88571 val_loss= 0.91158 val_acc= 0.81000 test_acc= 0.77900 time= 0.35818\n",
            "Epoch: 0105 train_loss= 0.68677 train_acc= 0.88571 val_loss= 0.90637 val_acc= 0.81000 test_acc= 0.77900 time= 0.36099\n",
            "Epoch: 0106 train_loss= 0.66816 train_acc= 0.88571 val_loss= 0.90129 val_acc= 0.81000 test_acc= 0.78300 time= 0.36413\n",
            "Epoch: 0107 train_loss= 0.69748 train_acc= 0.86429 val_loss= 0.89624 val_acc= 0.81000 test_acc= 0.78500 time= 0.35583\n",
            "Epoch: 0108 train_loss= 0.71382 train_acc= 0.86429 val_loss= 0.89129 val_acc= 0.81000 test_acc= 0.78600 time= 0.34815\n",
            "Epoch: 0109 train_loss= 0.67077 train_acc= 0.87857 val_loss= 0.88629 val_acc= 0.81000 test_acc= 0.78600 time= 0.36398\n",
            "Epoch: 0110 train_loss= 0.61024 train_acc= 0.90714 val_loss= 0.88135 val_acc= 0.81000 test_acc= 0.78500 time= 0.35488\n",
            "Epoch: 0111 train_loss= 0.64969 train_acc= 0.86429 val_loss= 0.87645 val_acc= 0.81333 test_acc= 0.78900 time= 0.36011\n",
            "Epoch: 0112 train_loss= 0.65081 train_acc= 0.88571 val_loss= 0.87170 val_acc= 0.81333 test_acc= 0.79000 time= 0.36282\n",
            "Epoch: 0113 train_loss= 0.65864 train_acc= 0.90714 val_loss= 0.86702 val_acc= 0.81333 test_acc= 0.79100 time= 0.35984\n",
            "Epoch: 0114 train_loss= 0.63147 train_acc= 0.87143 val_loss= 0.86256 val_acc= 0.81333 test_acc= 0.79700 time= 0.35786\n",
            "Epoch: 0115 train_loss= 0.64788 train_acc= 0.89286 val_loss= 0.85825 val_acc= 0.81667 test_acc= 0.79700 time= 0.36818\n",
            "Epoch: 0116 train_loss= 0.60169 train_acc= 0.90714 val_loss= 0.85401 val_acc= 0.81667 test_acc= 0.79900 time= 0.35412\n",
            "Epoch: 0117 train_loss= 0.65948 train_acc= 0.88571 val_loss= 0.84985 val_acc= 0.81667 test_acc= 0.79900 time= 0.35850\n",
            "Epoch: 0118 train_loss= 0.58422 train_acc= 0.91429 val_loss= 0.84574 val_acc= 0.81333 test_acc= 0.80300 time= 0.36345\n",
            "Epoch: 0119 train_loss= 0.64066 train_acc= 0.87857 val_loss= 0.84167 val_acc= 0.81333 test_acc= 0.80400 time= 0.35597\n",
            "Epoch: 0120 train_loss= 0.62339 train_acc= 0.92857 val_loss= 0.83770 val_acc= 0.81000 test_acc= 0.80500 time= 0.37602\n",
            "Epoch: 0121 train_loss= 0.59244 train_acc= 0.91429 val_loss= 0.83379 val_acc= 0.81333 test_acc= 0.80500 time= 0.35524\n",
            "Epoch: 0122 train_loss= 0.57632 train_acc= 0.91429 val_loss= 0.83005 val_acc= 0.81333 test_acc= 0.80600 time= 0.35155\n",
            "Epoch: 0123 train_loss= 0.56674 train_acc= 0.90000 val_loss= 0.82647 val_acc= 0.81333 test_acc= 0.80800 time= 0.36314\n",
            "Epoch: 0124 train_loss= 0.63515 train_acc= 0.87143 val_loss= 0.82299 val_acc= 0.81667 test_acc= 0.80900 time= 0.37103\n",
            "Epoch: 0125 train_loss= 0.57944 train_acc= 0.90714 val_loss= 0.81954 val_acc= 0.81333 test_acc= 0.81000 time= 0.35494\n",
            "Epoch: 0126 train_loss= 0.59491 train_acc= 0.92143 val_loss= 0.81633 val_acc= 0.81667 test_acc= 0.81000 time= 0.36311\n",
            "Epoch: 0127 train_loss= 0.57679 train_acc= 0.91429 val_loss= 0.81328 val_acc= 0.82000 test_acc= 0.80900 time= 0.35471\n",
            "Epoch: 0128 train_loss= 0.59407 train_acc= 0.90000 val_loss= 0.81037 val_acc= 0.82333 test_acc= 0.81300 time= 0.35199\n",
            "Epoch: 0129 train_loss= 0.56182 train_acc= 0.92143 val_loss= 0.80759 val_acc= 0.82333 test_acc= 0.81400 time= 0.36318\n",
            "Epoch: 0130 train_loss= 0.53993 train_acc= 0.92143 val_loss= 0.80488 val_acc= 0.82333 test_acc= 0.81600 time= 0.37369\n",
            "Epoch: 0131 train_loss= 0.56609 train_acc= 0.90000 val_loss= 0.80227 val_acc= 0.82667 test_acc= 0.81700 time= 0.36724\n",
            "Epoch: 0132 train_loss= 0.50843 train_acc= 0.95714 val_loss= 0.79970 val_acc= 0.83000 test_acc= 0.81800 time= 0.36794\n",
            "Epoch: 0133 train_loss= 0.57910 train_acc= 0.90714 val_loss= 0.79721 val_acc= 0.83333 test_acc= 0.81800 time= 0.35693\n",
            "Epoch: 0134 train_loss= 0.58550 train_acc= 0.92857 val_loss= 0.79459 val_acc= 0.83333 test_acc= 0.81700 time= 0.36781\n",
            "Epoch: 0135 train_loss= 0.54425 train_acc= 0.95000 val_loss= 0.79205 val_acc= 0.83333 test_acc= 0.81800 time= 0.36764\n",
            "Epoch: 0136 train_loss= 0.53797 train_acc= 0.94286 val_loss= 0.78951 val_acc= 0.83333 test_acc= 0.81900 time= 0.35540\n",
            "Epoch: 0137 train_loss= 0.53461 train_acc= 0.94286 val_loss= 0.78696 val_acc= 0.83333 test_acc= 0.82000 time= 0.37927\n",
            "Epoch: 0138 train_loss= 0.56587 train_acc= 0.94286 val_loss= 0.78449 val_acc= 0.83667 test_acc= 0.82100 time= 0.35898\n",
            "Epoch: 0139 train_loss= 0.53770 train_acc= 0.90714 val_loss= 0.78210 val_acc= 0.83667 test_acc= 0.82200 time= 0.35774\n",
            "Epoch: 0140 train_loss= 0.50091 train_acc= 0.92143 val_loss= 0.77975 val_acc= 0.83667 test_acc= 0.82100 time= 0.35993\n",
            "Epoch: 0141 train_loss= 0.51784 train_acc= 0.92857 val_loss= 0.77752 val_acc= 0.83667 test_acc= 0.82200 time= 0.35484\n",
            "Epoch: 0142 train_loss= 0.52825 train_acc= 0.90714 val_loss= 0.77530 val_acc= 0.83667 test_acc= 0.82200 time= 0.35971\n",
            "Epoch: 0143 train_loss= 0.53030 train_acc= 0.92857 val_loss= 0.77320 val_acc= 0.84000 test_acc= 0.82200 time= 0.36847\n",
            "Epoch: 0144 train_loss= 0.55617 train_acc= 0.90714 val_loss= 0.77107 val_acc= 0.84000 test_acc= 0.82100 time= 0.35647\n",
            "Epoch: 0145 train_loss= 0.54396 train_acc= 0.91429 val_loss= 0.76899 val_acc= 0.84000 test_acc= 0.82000 time= 0.34692\n",
            "Epoch: 0146 train_loss= 0.50359 train_acc= 0.92143 val_loss= 0.76690 val_acc= 0.84000 test_acc= 0.82100 time= 0.38187\n",
            "Epoch: 0147 train_loss= 0.48716 train_acc= 0.94286 val_loss= 0.76484 val_acc= 0.84000 test_acc= 0.82300 time= 0.36617\n",
            "Epoch: 0148 train_loss= 0.47374 train_acc= 0.95000 val_loss= 0.76274 val_acc= 0.84000 test_acc= 0.82400 time= 0.35793\n",
            "Epoch: 0149 train_loss= 0.48519 train_acc= 0.93571 val_loss= 0.76062 val_acc= 0.84000 test_acc= 0.82600 time= 0.37413\n",
            "Epoch: 0150 train_loss= 0.45775 train_acc= 0.95000 val_loss= 0.75858 val_acc= 0.84000 test_acc= 0.82700 time= 0.35835\n",
            "Epoch: 0151 train_loss= 0.49173 train_acc= 0.94286 val_loss= 0.75655 val_acc= 0.84000 test_acc= 0.82800 time= 0.37083\n",
            "Epoch: 0152 train_loss= 0.50772 train_acc= 0.94286 val_loss= 0.75453 val_acc= 0.84000 test_acc= 0.82900 time= 0.35677\n",
            "Epoch: 0153 train_loss= 0.45692 train_acc= 0.94286 val_loss= 0.75259 val_acc= 0.84000 test_acc= 0.82900 time= 0.36636\n",
            "Epoch: 0154 train_loss= 0.48992 train_acc= 0.95000 val_loss= 0.75055 val_acc= 0.84000 test_acc= 0.82800 time= 0.36360\n",
            "Epoch: 0155 train_loss= 0.43768 train_acc= 0.96429 val_loss= 0.74860 val_acc= 0.84333 test_acc= 0.82900 time= 0.36334\n",
            "Epoch: 0156 train_loss= 0.46538 train_acc= 0.96429 val_loss= 0.74673 val_acc= 0.84333 test_acc= 0.82800 time= 0.35308\n",
            "Epoch: 0157 train_loss= 0.48624 train_acc= 0.96429 val_loss= 0.74491 val_acc= 0.84333 test_acc= 0.82800 time= 0.36363\n",
            "Epoch: 0158 train_loss= 0.45762 train_acc= 0.92143 val_loss= 0.74314 val_acc= 0.84333 test_acc= 0.82800 time= 0.35781\n",
            "Epoch: 0159 train_loss= 0.48289 train_acc= 0.92143 val_loss= 0.74143 val_acc= 0.84333 test_acc= 0.82800 time= 0.35337\n",
            "Epoch: 0160 train_loss= 0.49071 train_acc= 0.92857 val_loss= 0.73975 val_acc= 0.84333 test_acc= 0.82900 time= 0.37763\n",
            "Epoch: 0161 train_loss= 0.49504 train_acc= 0.91429 val_loss= 0.73798 val_acc= 0.84333 test_acc= 0.82900 time= 0.36510\n",
            "Epoch: 0162 train_loss= 0.45787 train_acc= 0.94286 val_loss= 0.73625 val_acc= 0.84333 test_acc= 0.83000 time= 0.36018\n",
            "Epoch: 0163 train_loss= 0.47890 train_acc= 0.92143 val_loss= 0.73461 val_acc= 0.84000 test_acc= 0.83400 time= 0.37574\n",
            "Epoch: 0164 train_loss= 0.46694 train_acc= 0.93571 val_loss= 0.73309 val_acc= 0.84000 test_acc= 0.83500 time= 0.35762\n",
            "Epoch: 0165 train_loss= 0.46895 train_acc= 0.94286 val_loss= 0.73161 val_acc= 0.83667 test_acc= 0.83600 time= 0.36245\n",
            "Epoch: 0166 train_loss= 0.45965 train_acc= 0.93571 val_loss= 0.73005 val_acc= 0.84000 test_acc= 0.83700 time= 0.37059\n",
            "Epoch: 0167 train_loss= 0.44242 train_acc= 0.94286 val_loss= 0.72842 val_acc= 0.84000 test_acc= 0.83700 time= 0.35010\n",
            "Epoch: 0168 train_loss= 0.46170 train_acc= 0.94286 val_loss= 0.72675 val_acc= 0.84000 test_acc= 0.83700 time= 0.36966\n",
            "Epoch: 0169 train_loss= 0.45169 train_acc= 0.92857 val_loss= 0.72509 val_acc= 0.84000 test_acc= 0.83900 time= 0.35755\n",
            "Epoch: 0170 train_loss= 0.42482 train_acc= 0.97143 val_loss= 0.72349 val_acc= 0.84000 test_acc= 0.83800 time= 0.35936\n",
            "Epoch: 0171 train_loss= 0.44313 train_acc= 0.94286 val_loss= 0.72202 val_acc= 0.84000 test_acc= 0.83900 time= 0.36524\n",
            "Epoch: 0172 train_loss= 0.47208 train_acc= 0.95714 val_loss= 0.72060 val_acc= 0.84000 test_acc= 0.84200 time= 0.36349\n",
            "Epoch: 0173 train_loss= 0.40378 train_acc= 0.95000 val_loss= 0.71925 val_acc= 0.84000 test_acc= 0.84200 time= 0.35765\n",
            "Epoch: 0174 train_loss= 0.41603 train_acc= 0.95000 val_loss= 0.71798 val_acc= 0.83667 test_acc= 0.84200 time= 0.36926\n",
            "Epoch: 0175 train_loss= 0.41537 train_acc= 0.95000 val_loss= 0.71678 val_acc= 0.83333 test_acc= 0.84300 time= 0.36251\n",
            "Epoch: 0176 train_loss= 0.47136 train_acc= 0.92857 val_loss= 0.71578 val_acc= 0.83333 test_acc= 0.84300 time= 0.36099\n",
            "Epoch: 0177 train_loss= 0.44745 train_acc= 0.92857 val_loss= 0.71477 val_acc= 0.83000 test_acc= 0.84300 time= 0.37441\n",
            "Epoch: 0178 train_loss= 0.40311 train_acc= 0.95000 val_loss= 0.71394 val_acc= 0.83000 test_acc= 0.84300 time= 0.36438\n",
            "Epoch: 0179 train_loss= 0.39380 train_acc= 0.97857 val_loss= 0.71316 val_acc= 0.83000 test_acc= 0.84300 time= 0.37063\n",
            "Epoch: 0180 train_loss= 0.44075 train_acc= 0.95714 val_loss= 0.71241 val_acc= 0.83000 test_acc= 0.84200 time= 0.37040\n",
            "Epoch: 0181 train_loss= 0.44549 train_acc= 0.92143 val_loss= 0.71154 val_acc= 0.83000 test_acc= 0.84300 time= 0.37291\n",
            "Epoch: 0182 train_loss= 0.40185 train_acc= 0.95000 val_loss= 0.71078 val_acc= 0.83000 test_acc= 0.84300 time= 0.37506\n",
            "Epoch: 0183 train_loss= 0.38183 train_acc= 0.98571 val_loss= 0.71009 val_acc= 0.83000 test_acc= 0.84300 time= 0.36798\n",
            "Epoch: 0184 train_loss= 0.40487 train_acc= 0.94286 val_loss= 0.70947 val_acc= 0.83333 test_acc= 0.84200 time= 0.36010\n",
            "Epoch: 0185 train_loss= 0.41894 train_acc= 0.95714 val_loss= 0.70887 val_acc= 0.83333 test_acc= 0.84300 time= 0.37161\n",
            "Epoch: 0186 train_loss= 0.36641 train_acc= 0.95000 val_loss= 0.70826 val_acc= 0.83333 test_acc= 0.84200 time= 0.35546\n",
            "Epoch: 0187 train_loss= 0.41503 train_acc= 0.93571 val_loss= 0.70776 val_acc= 0.83333 test_acc= 0.84200 time= 0.35576\n",
            "Epoch: 0188 train_loss= 0.39241 train_acc= 0.95000 val_loss= 0.70724 val_acc= 0.83333 test_acc= 0.84200 time= 0.36735\n",
            "Epoch: 0189 train_loss= 0.38957 train_acc= 0.97143 val_loss= 0.70667 val_acc= 0.83333 test_acc= 0.84200 time= 0.35505\n",
            "Epoch: 0190 train_loss= 0.39860 train_acc= 0.96429 val_loss= 0.70612 val_acc= 0.83333 test_acc= 0.84200 time= 0.36674\n",
            "Epoch: 0191 train_loss= 0.45117 train_acc= 0.95000 val_loss= 0.70559 val_acc= 0.83333 test_acc= 0.84000 time= 0.37244\n",
            "Epoch: 0192 train_loss= 0.41658 train_acc= 0.93571 val_loss= 0.70507 val_acc= 0.83333 test_acc= 0.84100 time= 0.37046\n",
            "Epoch: 0193 train_loss= 0.40608 train_acc= 0.94286 val_loss= 0.70446 val_acc= 0.83333 test_acc= 0.84200 time= 0.36118\n",
            "Epoch: 0194 train_loss= 0.39073 train_acc= 0.95000 val_loss= 0.70395 val_acc= 0.83333 test_acc= 0.84100 time= 0.36703\n",
            "Epoch: 0195 train_loss= 0.40301 train_acc= 0.97143 val_loss= 0.70330 val_acc= 0.83333 test_acc= 0.84300 time= 0.37028\n",
            "Epoch: 0196 train_loss= 0.37984 train_acc= 0.96429 val_loss= 0.70263 val_acc= 0.83333 test_acc= 0.84300 time= 0.37405\n",
            "Epoch: 0197 train_loss= 0.34514 train_acc= 0.97857 val_loss= 0.70190 val_acc= 0.83333 test_acc= 0.84300 time= 0.35738\n",
            "Epoch: 0198 train_loss= 0.40902 train_acc= 0.95000 val_loss= 0.70129 val_acc= 0.83333 test_acc= 0.84500 time= 0.35668\n",
            "Epoch: 0199 train_loss= 0.37023 train_acc= 0.95000 val_loss= 0.70052 val_acc= 0.83333 test_acc= 0.84500 time= 0.36898\n",
            "Epoch: 0200 train_loss= 0.38301 train_acc= 0.91429 val_loss= 0.69973 val_acc= 0.83333 test_acc= 0.84600 time= 0.35811\n",
            "Epoch: 0201 train_loss= 0.40100 train_acc= 0.94286 val_loss= 0.69899 val_acc= 0.83000 test_acc= 0.84900 time= 0.35953\n",
            "Epoch: 0202 train_loss= 0.40985 train_acc= 0.92857 val_loss= 0.69829 val_acc= 0.83000 test_acc= 0.84700 time= 0.37113\n",
            "Epoch: 0203 train_loss= 0.37972 train_acc= 0.97143 val_loss= 0.69752 val_acc= 0.83000 test_acc= 0.84700 time= 0.36116\n",
            "Epoch: 0204 train_loss= 0.34574 train_acc= 0.96429 val_loss= 0.69669 val_acc= 0.83000 test_acc= 0.84700 time= 0.36069\n",
            "Epoch: 0205 train_loss= 0.38452 train_acc= 0.95000 val_loss= 0.69603 val_acc= 0.83000 test_acc= 0.84900 time= 0.36113\n",
            "Epoch: 0206 train_loss= 0.36352 train_acc= 0.97857 val_loss= 0.69546 val_acc= 0.83000 test_acc= 0.84900 time= 0.36148\n",
            "Epoch: 0207 train_loss= 0.35493 train_acc= 0.97143 val_loss= 0.69486 val_acc= 0.83000 test_acc= 0.85100 time= 0.35781\n",
            "Epoch: 0208 train_loss= 0.36300 train_acc= 0.95714 val_loss= 0.69425 val_acc= 0.83000 test_acc= 0.85100 time= 0.38522\n",
            "Epoch: 0209 train_loss= 0.33075 train_acc= 0.97857 val_loss= 0.69364 val_acc= 0.83000 test_acc= 0.85100 time= 0.35634\n",
            "Epoch: 0210 train_loss= 0.34245 train_acc= 0.97143 val_loss= 0.69310 val_acc= 0.82667 test_acc= 0.85100 time= 0.36071\n",
            "Epoch: 0211 train_loss= 0.35753 train_acc= 0.95000 val_loss= 0.69259 val_acc= 0.82667 test_acc= 0.85100 time= 0.35617\n",
            "Epoch: 0212 train_loss= 0.35646 train_acc= 0.95000 val_loss= 0.69207 val_acc= 0.82667 test_acc= 0.85100 time= 0.35386\n",
            "Epoch: 0213 train_loss= 0.34677 train_acc= 0.95000 val_loss= 0.69147 val_acc= 0.82667 test_acc= 0.85000 time= 0.36401\n",
            "Epoch: 0214 train_loss= 0.38801 train_acc= 0.95000 val_loss= 0.69089 val_acc= 0.82667 test_acc= 0.85000 time= 0.35783\n",
            "Epoch: 0215 train_loss= 0.33920 train_acc= 0.97857 val_loss= 0.69020 val_acc= 0.82667 test_acc= 0.85000 time= 0.35453\n",
            "Epoch: 0216 train_loss= 0.34809 train_acc= 0.96429 val_loss= 0.68950 val_acc= 0.82667 test_acc= 0.85000 time= 0.36353\n",
            "Epoch: 0217 train_loss= 0.34916 train_acc= 0.97143 val_loss= 0.68886 val_acc= 0.82667 test_acc= 0.85000 time= 0.35645\n",
            "Epoch: 0218 train_loss= 0.38200 train_acc= 0.92857 val_loss= 0.68823 val_acc= 0.82667 test_acc= 0.85100 time= 0.35325\n",
            "Epoch: 0219 train_loss= 0.38243 train_acc= 0.96429 val_loss= 0.68755 val_acc= 0.82667 test_acc= 0.84900 time= 0.36527\n",
            "Epoch: 0220 train_loss= 0.35474 train_acc= 0.96429 val_loss= 0.68681 val_acc= 0.82667 test_acc= 0.85000 time= 0.35834\n",
            "Epoch: 0221 train_loss= 0.36399 train_acc= 0.97857 val_loss= 0.68608 val_acc= 0.82667 test_acc= 0.85100 time= 0.35086\n",
            "Epoch: 0222 train_loss= 0.35842 train_acc= 0.95714 val_loss= 0.68540 val_acc= 0.82667 test_acc= 0.85100 time= 0.36244\n",
            "Epoch: 0223 train_loss= 0.37684 train_acc= 0.96429 val_loss= 0.68483 val_acc= 0.82667 test_acc= 0.85200 time= 0.35367\n",
            "Epoch: 0224 train_loss= 0.34858 train_acc= 0.94286 val_loss= 0.68423 val_acc= 0.82667 test_acc= 0.85100 time= 0.35830\n",
            "Epoch: 0225 train_loss= 0.32695 train_acc= 0.95714 val_loss= 0.68370 val_acc= 0.82667 test_acc= 0.85000 time= 0.36531\n",
            "Epoch: 0226 train_loss= 0.34191 train_acc= 0.95714 val_loss= 0.68326 val_acc= 0.82667 test_acc= 0.85000 time= 0.35039\n",
            "Epoch: 0227 train_loss= 0.34417 train_acc= 0.97143 val_loss= 0.68279 val_acc= 0.82667 test_acc= 0.84900 time= 0.35416\n",
            "Epoch: 0228 train_loss= 0.35106 train_acc= 0.95714 val_loss= 0.68237 val_acc= 0.82667 test_acc= 0.85000 time= 0.37357\n",
            "Epoch: 0229 train_loss= 0.35220 train_acc= 0.95714 val_loss= 0.68200 val_acc= 0.82667 test_acc= 0.85000 time= 0.36401\n",
            "Epoch: 0230 train_loss= 0.35981 train_acc= 0.95714 val_loss= 0.68167 val_acc= 0.82667 test_acc= 0.85100 time= 0.34509\n",
            "Epoch: 0231 train_loss= 0.33343 train_acc= 0.95714 val_loss= 0.68140 val_acc= 0.82667 test_acc= 0.85100 time= 0.36549\n",
            "Epoch: 0232 train_loss= 0.34475 train_acc= 0.95000 val_loss= 0.68115 val_acc= 0.82667 test_acc= 0.85200 time= 0.35369\n",
            "Epoch: 0233 train_loss= 0.36475 train_acc= 0.95714 val_loss= 0.68098 val_acc= 0.82667 test_acc= 0.85200 time= 0.36842\n",
            "Epoch: 0234 train_loss= 0.32022 train_acc= 0.96429 val_loss= 0.68075 val_acc= 0.82667 test_acc= 0.85300 time= 0.36205\n",
            "Epoch: 0235 train_loss= 0.32784 train_acc= 0.96429 val_loss= 0.68059 val_acc= 0.82667 test_acc= 0.85300 time= 0.35233\n",
            "Epoch: 0236 train_loss= 0.39926 train_acc= 0.93571 val_loss= 0.68039 val_acc= 0.82667 test_acc= 0.85300 time= 0.36758\n",
            "Epoch: 0237 train_loss= 0.36575 train_acc= 0.92857 val_loss= 0.68026 val_acc= 0.82667 test_acc= 0.85300 time= 0.35319\n",
            "Epoch: 0238 train_loss= 0.32948 train_acc= 0.95714 val_loss= 0.68018 val_acc= 0.82667 test_acc= 0.85300 time= 0.36093\n",
            "Epoch: 0239 train_loss= 0.34121 train_acc= 0.97143 val_loss= 0.68016 val_acc= 0.82667 test_acc= 0.85300 time= 0.36977\n",
            "Epoch: 0240 train_loss= 0.35278 train_acc= 0.96429 val_loss= 0.68000 val_acc= 0.82333 test_acc= 0.85200 time= 0.35226\n",
            "Epoch: 0241 train_loss= 0.34423 train_acc= 0.94286 val_loss= 0.67995 val_acc= 0.82333 test_acc= 0.85100 time= 0.38013\n",
            "Epoch: 0242 train_loss= 0.38120 train_acc= 0.95000 val_loss= 0.67996 val_acc= 0.82333 test_acc= 0.85000 time= 0.37178\n",
            "Epoch: 0243 train_loss= 0.33736 train_acc= 0.97143 val_loss= 0.67986 val_acc= 0.82333 test_acc= 0.85100 time= 0.35701\n",
            "Epoch: 0244 train_loss= 0.35385 train_acc= 0.95714 val_loss= 0.67978 val_acc= 0.82000 test_acc= 0.85100 time= 0.40286\n",
            "Epoch: 0245 train_loss= 0.38101 train_acc= 0.92857 val_loss= 0.67978 val_acc= 0.82000 test_acc= 0.84900 time= 0.37996\n",
            "Epoch: 0246 train_loss= 0.32544 train_acc= 0.97143 val_loss= 0.67978 val_acc= 0.81667 test_acc= 0.84900 time= 0.36099\n",
            "Epoch: 0247 train_loss= 0.38654 train_acc= 0.95714 val_loss= 0.67974 val_acc= 0.81667 test_acc= 0.84900 time= 0.36202\n",
            "Epoch: 0248 train_loss= 0.34129 train_acc= 0.95000 val_loss= 0.67964 val_acc= 0.81667 test_acc= 0.84900 time= 0.35546\n",
            "Epoch: 0249 train_loss= 0.34192 train_acc= 0.97143 val_loss= 0.67933 val_acc= 0.81667 test_acc= 0.84900 time= 0.36270\n",
            "Epoch: 0250 train_loss= 0.28744 train_acc= 0.95000 val_loss= 0.67897 val_acc= 0.81667 test_acc= 0.84900 time= 0.36392\n",
            "Epoch: 0251 train_loss= 0.34945 train_acc= 0.97143 val_loss= 0.67860 val_acc= 0.81667 test_acc= 0.84900 time= 0.36134\n",
            "Epoch: 0252 train_loss= 0.29632 train_acc= 0.97143 val_loss= 0.67819 val_acc= 0.82000 test_acc= 0.84900 time= 0.35145\n",
            "Epoch: 0253 train_loss= 0.33327 train_acc= 0.95714 val_loss= 0.67775 val_acc= 0.82000 test_acc= 0.84900 time= 0.37562\n",
            "Epoch: 0254 train_loss= 0.35036 train_acc= 0.94286 val_loss= 0.67728 val_acc= 0.82000 test_acc= 0.85000 time= 0.36629\n",
            "Epoch: 0255 train_loss= 0.34167 train_acc= 0.95000 val_loss= 0.67677 val_acc= 0.82000 test_acc= 0.84900 time= 0.35891\n",
            "Epoch: 0256 train_loss= 0.29302 train_acc= 0.96429 val_loss= 0.67632 val_acc= 0.82333 test_acc= 0.84900 time= 0.35786\n",
            "Epoch: 0257 train_loss= 0.33815 train_acc= 0.95000 val_loss= 0.67587 val_acc= 0.82333 test_acc= 0.84800 time= 0.35963\n",
            "Epoch: 0258 train_loss= 0.34124 train_acc= 0.96429 val_loss= 0.67535 val_acc= 0.82667 test_acc= 0.84900 time= 0.35942\n",
            "Epoch: 0259 train_loss= 0.32882 train_acc= 0.97857 val_loss= 0.67495 val_acc= 0.82667 test_acc= 0.84900 time= 0.37676\n",
            "Epoch: 0260 train_loss= 0.32077 train_acc= 0.97143 val_loss= 0.67454 val_acc= 0.82667 test_acc= 0.84800 time= 0.36843\n",
            "Epoch: 0261 train_loss= 0.30708 train_acc= 0.95000 val_loss= 0.67425 val_acc= 0.82667 test_acc= 0.84900 time= 0.36206\n",
            "Epoch: 0262 train_loss= 0.30411 train_acc= 0.95000 val_loss= 0.67392 val_acc= 0.83000 test_acc= 0.85000 time= 0.37016\n",
            "Epoch: 0263 train_loss= 0.35447 train_acc= 0.93571 val_loss= 0.67349 val_acc= 0.83333 test_acc= 0.85100 time= 0.35919\n",
            "Epoch: 0264 train_loss= 0.29707 train_acc= 0.94286 val_loss= 0.67306 val_acc= 0.83333 test_acc= 0.85100 time= 0.37011\n",
            "Epoch: 0265 train_loss= 0.34670 train_acc= 0.95000 val_loss= 0.67250 val_acc= 0.83333 test_acc= 0.85100 time= 0.36202\n",
            "Epoch: 0266 train_loss= 0.36147 train_acc= 0.96429 val_loss= 0.67189 val_acc= 0.83333 test_acc= 0.85200 time= 0.35545\n",
            "Epoch: 0267 train_loss= 0.29116 train_acc= 0.97857 val_loss= 0.67132 val_acc= 0.83333 test_acc= 0.85400 time= 0.37080\n",
            "Epoch: 0268 train_loss= 0.30006 train_acc= 0.95000 val_loss= 0.67083 val_acc= 0.83333 test_acc= 0.85400 time= 0.35580\n",
            "Epoch: 0269 train_loss= 0.29265 train_acc= 0.95000 val_loss= 0.67038 val_acc= 0.83333 test_acc= 0.85400 time= 0.35766\n",
            "Epoch: 0270 train_loss= 0.32948 train_acc= 0.95000 val_loss= 0.66996 val_acc= 0.83333 test_acc= 0.85400 time= 0.38792\n",
            "Epoch: 0271 train_loss= 0.29209 train_acc= 0.98571 val_loss= 0.66961 val_acc= 0.83000 test_acc= 0.85300 time= 0.36005\n",
            "Epoch: 0272 train_loss= 0.36109 train_acc= 0.94286 val_loss= 0.66940 val_acc= 0.83000 test_acc= 0.85300 time= 0.35851\n",
            "Epoch: 0273 train_loss= 0.30272 train_acc= 0.95000 val_loss= 0.66922 val_acc= 0.83000 test_acc= 0.85300 time= 0.36990\n",
            "Epoch: 0274 train_loss= 0.33919 train_acc= 0.96429 val_loss= 0.66907 val_acc= 0.83000 test_acc= 0.85200 time= 0.36223\n",
            "Epoch: 0275 train_loss= 0.27775 train_acc= 0.97857 val_loss= 0.66902 val_acc= 0.82333 test_acc= 0.85300 time= 0.36134\n",
            "Epoch: 0276 train_loss= 0.28498 train_acc= 0.96429 val_loss= 0.66889 val_acc= 0.82333 test_acc= 0.85400 time= 0.37529\n",
            "Epoch: 0277 train_loss= 0.33230 train_acc= 0.95714 val_loss= 0.66871 val_acc= 0.82333 test_acc= 0.85400 time= 0.35156\n",
            "Epoch: 0278 train_loss= 0.32982 train_acc= 0.96429 val_loss= 0.66857 val_acc= 0.82333 test_acc= 0.85200 time= 0.36444\n",
            "Epoch: 0279 train_loss= 0.31942 train_acc= 0.98571 val_loss= 0.66844 val_acc= 0.82333 test_acc= 0.85200 time= 0.35591\n",
            "Epoch: 0280 train_loss= 0.31878 train_acc= 0.96429 val_loss= 0.66827 val_acc= 0.82333 test_acc= 0.85200 time= 0.36066\n",
            "Epoch: 0281 train_loss= 0.34350 train_acc= 0.96429 val_loss= 0.66804 val_acc= 0.82333 test_acc= 0.85300 time= 0.36748\n",
            "Epoch: 0282 train_loss= 0.28848 train_acc= 0.97857 val_loss= 0.66783 val_acc= 0.82333 test_acc= 0.85400 time= 0.35822\n",
            "Epoch: 0283 train_loss= 0.27858 train_acc= 0.98571 val_loss= 0.66765 val_acc= 0.82333 test_acc= 0.85300 time= 0.35793\n",
            "Epoch: 0284 train_loss= 0.31145 train_acc= 0.97143 val_loss= 0.66749 val_acc= 0.82667 test_acc= 0.85100 time= 0.37878\n",
            "Epoch: 0285 train_loss= 0.29838 train_acc= 0.97143 val_loss= 0.66721 val_acc= 0.82667 test_acc= 0.85200 time= 0.35549\n",
            "Epoch: 0286 train_loss= 0.31687 train_acc= 0.95000 val_loss= 0.66714 val_acc= 0.83000 test_acc= 0.85200 time= 0.35576\n",
            "Epoch: 0287 train_loss= 0.31506 train_acc= 0.96429 val_loss= 0.66707 val_acc= 0.83000 test_acc= 0.85200 time= 0.37615\n",
            "Epoch: 0288 train_loss= 0.28992 train_acc= 0.97857 val_loss= 0.66698 val_acc= 0.83000 test_acc= 0.85200 time= 0.35823\n",
            "Epoch: 0289 train_loss= 0.29252 train_acc= 0.95000 val_loss= 0.66686 val_acc= 0.83000 test_acc= 0.85400 time= 0.35929\n",
            "Epoch: 0290 train_loss= 0.31477 train_acc= 0.95714 val_loss= 0.66688 val_acc= 0.83000 test_acc= 0.85500 time= 0.36916\n",
            "Epoch: 0291 train_loss= 0.26994 train_acc= 0.99286 val_loss= 0.66687 val_acc= 0.83000 test_acc= 0.85500 time= 0.35267\n",
            "Epoch: 0292 train_loss= 0.29102 train_acc= 0.95714 val_loss= 0.66679 val_acc= 0.83000 test_acc= 0.85400 time= 0.35788\n",
            "Epoch: 0293 train_loss= 0.30632 train_acc= 0.97143 val_loss= 0.66670 val_acc= 0.83000 test_acc= 0.85400 time= 0.36587\n",
            "Epoch: 0294 train_loss= 0.31087 train_acc= 0.97857 val_loss= 0.66664 val_acc= 0.83000 test_acc= 0.85400 time= 0.35275\n",
            "Epoch: 0295 train_loss= 0.25884 train_acc= 0.97143 val_loss= 0.66656 val_acc= 0.83000 test_acc= 0.85300 time= 0.36690\n",
            "Epoch: 0296 train_loss= 0.29650 train_acc= 0.97143 val_loss= 0.66640 val_acc= 0.83000 test_acc= 0.85400 time= 0.36307\n",
            "Epoch: 0297 train_loss= 0.31728 train_acc= 0.96429 val_loss= 0.66625 val_acc= 0.83000 test_acc= 0.85400 time= 0.35992\n",
            "Epoch: 0298 train_loss= 0.30243 train_acc= 0.95714 val_loss= 0.66614 val_acc= 0.83000 test_acc= 0.85500 time= 0.39115\n",
            "Epoch: 0299 train_loss= 0.31884 train_acc= 0.94286 val_loss= 0.66620 val_acc= 0.83000 test_acc= 0.85400 time= 0.36346\n",
            "Epoch: 0300 train_loss= 0.26930 train_acc= 0.97143 val_loss= 0.66636 val_acc= 0.83000 test_acc= 0.85500 time= 0.35815\n",
            "Epoch: 0301 train_loss= 0.28961 train_acc= 0.94286 val_loss= 0.66660 val_acc= 0.83000 test_acc= 0.85600 time= 0.37041\n",
            "Epoch: 0302 train_loss= 0.29080 train_acc= 0.95000 val_loss= 0.66676 val_acc= 0.83000 test_acc= 0.85600 time= 0.35845\n",
            "Epoch: 0303 train_loss= 0.31043 train_acc= 0.96429 val_loss= 0.66685 val_acc= 0.83333 test_acc= 0.85600 time= 0.37080\n",
            "Epoch: 0304 train_loss= 0.28818 train_acc= 0.95000 val_loss= 0.66689 val_acc= 0.83333 test_acc= 0.85400 time= 0.37755\n",
            "Epoch: 0305 train_loss= 0.31258 train_acc= 0.95714 val_loss= 0.66699 val_acc= 0.83000 test_acc= 0.85600 time= 0.35967\n",
            "Epoch: 0306 train_loss= 0.24302 train_acc= 0.98571 val_loss= 0.66699 val_acc= 0.83000 test_acc= 0.85600 time= 0.35835\n",
            "Epoch: 0307 train_loss= 0.28209 train_acc= 0.98571 val_loss= 0.66682 val_acc= 0.83000 test_acc= 0.85500 time= 0.36579\n",
            "Epoch: 0308 train_loss= 0.30074 train_acc= 0.96429 val_loss= 0.66665 val_acc= 0.83000 test_acc= 0.85400 time= 0.36116\n",
            "Epoch: 0309 train_loss= 0.25951 train_acc= 0.97143 val_loss= 0.66645 val_acc= 0.83000 test_acc= 0.85400 time= 0.37367\n",
            "Epoch: 0310 train_loss= 0.26658 train_acc= 0.97143 val_loss= 0.66618 val_acc= 0.83000 test_acc= 0.85500 time= 0.35602\n",
            "Epoch: 0311 train_loss= 0.26405 train_acc= 0.97857 val_loss= 0.66588 val_acc= 0.83000 test_acc= 0.85500 time= 0.36432\n",
            "Epoch: 0312 train_loss= 0.29800 train_acc= 0.97143 val_loss= 0.66558 val_acc= 0.83000 test_acc= 0.85500 time= 0.36765\n",
            "Epoch: 0313 train_loss= 0.27136 train_acc= 0.97143 val_loss= 0.66539 val_acc= 0.83000 test_acc= 0.85500 time= 0.36466\n",
            "Epoch: 0314 train_loss= 0.31651 train_acc= 0.93571 val_loss= 0.66516 val_acc= 0.83000 test_acc= 0.85500 time= 0.35771\n",
            "Epoch: 0315 train_loss= 0.27527 train_acc= 0.97143 val_loss= 0.66498 val_acc= 0.83000 test_acc= 0.85500 time= 0.35675\n",
            "Epoch: 0316 train_loss= 0.28527 train_acc= 0.97143 val_loss= 0.66485 val_acc= 0.83000 test_acc= 0.85500 time= 0.35358\n",
            "Epoch: 0317 train_loss= 0.27824 train_acc= 0.96429 val_loss= 0.66469 val_acc= 0.83333 test_acc= 0.85600 time= 0.35081\n",
            "Epoch: 0318 train_loss= 0.27784 train_acc= 0.97857 val_loss= 0.66440 val_acc= 0.83333 test_acc= 0.85500 time= 0.36137\n",
            "Epoch: 0319 train_loss= 0.26701 train_acc= 0.97143 val_loss= 0.66421 val_acc= 0.83667 test_acc= 0.85700 time= 0.35800\n",
            "Epoch: 0320 train_loss= 0.28966 train_acc= 0.97857 val_loss= 0.66393 val_acc= 0.83667 test_acc= 0.85700 time= 0.35548\n",
            "Epoch: 0321 train_loss= 0.29910 train_acc= 0.97143 val_loss= 0.66359 val_acc= 0.83667 test_acc= 0.85700 time= 0.35922\n",
            "Epoch: 0322 train_loss= 0.28940 train_acc= 0.97143 val_loss= 0.66336 val_acc= 0.83667 test_acc= 0.85900 time= 0.35502\n",
            "Epoch: 0323 train_loss= 0.29527 train_acc= 0.95000 val_loss= 0.66318 val_acc= 0.83667 test_acc= 0.85800 time= 0.35735\n",
            "Epoch: 0324 train_loss= 0.29399 train_acc= 0.96429 val_loss= 0.66304 val_acc= 0.83667 test_acc= 0.85900 time= 0.36441\n",
            "Epoch: 0325 train_loss= 0.27634 train_acc= 0.96429 val_loss= 0.66297 val_acc= 0.83667 test_acc= 0.85700 time= 0.35703\n",
            "Epoch: 0326 train_loss= 0.28660 train_acc= 0.97143 val_loss= 0.66288 val_acc= 0.83333 test_acc= 0.85800 time= 0.34970\n",
            "Epoch: 0327 train_loss= 0.29709 train_acc= 0.96429 val_loss= 0.66270 val_acc= 0.83333 test_acc= 0.85900 time= 0.38851\n",
            "Epoch: 0328 train_loss= 0.33412 train_acc= 0.95714 val_loss= 0.66256 val_acc= 0.83333 test_acc= 0.85800 time= 0.35375\n",
            "Epoch: 0329 train_loss= 0.30091 train_acc= 0.97143 val_loss= 0.66238 val_acc= 0.83333 test_acc= 0.85800 time= 0.36431\n",
            "Epoch: 0330 train_loss= 0.29709 train_acc= 0.95714 val_loss= 0.66219 val_acc= 0.83333 test_acc= 0.85700 time= 0.35299\n",
            "Epoch: 0331 train_loss= 0.29683 train_acc= 0.96429 val_loss= 0.66194 val_acc= 0.83333 test_acc= 0.85700 time= 0.35239\n",
            "Epoch: 0332 train_loss= 0.28431 train_acc= 0.95000 val_loss= 0.66181 val_acc= 0.83333 test_acc= 0.85500 time= 0.36383\n",
            "Epoch: 0333 train_loss= 0.30920 train_acc= 0.97143 val_loss= 0.66171 val_acc= 0.83333 test_acc= 0.85600 time= 0.35897\n",
            "Epoch: 0334 train_loss= 0.24749 train_acc= 0.98571 val_loss= 0.66165 val_acc= 0.83333 test_acc= 0.85600 time= 0.36090\n",
            "Epoch: 0335 train_loss= 0.25363 train_acc= 0.96429 val_loss= 0.66163 val_acc= 0.83333 test_acc= 0.85600 time= 0.37265\n",
            "Epoch: 0336 train_loss= 0.26672 train_acc= 0.97143 val_loss= 0.66179 val_acc= 0.83333 test_acc= 0.85600 time= 0.35662\n",
            "Epoch: 0337 train_loss= 0.29930 train_acc= 0.96429 val_loss= 0.66186 val_acc= 0.83333 test_acc= 0.85700 time= 0.37929\n",
            "Epoch: 0338 train_loss= 0.34925 train_acc= 0.95714 val_loss= 0.66197 val_acc= 0.83333 test_acc= 0.85500 time= 0.37781\n",
            "Epoch: 0339 train_loss= 0.29073 train_acc= 0.97143 val_loss= 0.66207 val_acc= 0.83333 test_acc= 0.85300 time= 0.34899\n",
            "Epoch: 0340 train_loss= 0.28595 train_acc= 0.96429 val_loss= 0.66209 val_acc= 0.83000 test_acc= 0.85300 time= 0.36300\n",
            "Epoch: 0341 train_loss= 0.31123 train_acc= 0.95714 val_loss= 0.66229 val_acc= 0.83000 test_acc= 0.85300 time= 0.40012\n",
            "Epoch: 0342 train_loss= 0.28376 train_acc= 0.97857 val_loss= 0.66243 val_acc= 0.83000 test_acc= 0.85300 time= 0.55781\n",
            "Epoch: 0343 train_loss= 0.28040 train_acc= 0.97857 val_loss= 0.66272 val_acc= 0.82667 test_acc= 0.85400 time= 0.76049\n",
            "Epoch: 0344 train_loss= 0.28301 train_acc= 0.97143 val_loss= 0.66295 val_acc= 0.82667 test_acc= 0.85400 time= 0.65680\n",
            "Epoch: 0345 train_loss= 0.29353 train_acc= 0.96429 val_loss= 0.66333 val_acc= 0.82667 test_acc= 0.85400 time= 0.60241\n",
            "Epoch: 0346 train_loss= 0.26886 train_acc= 0.97143 val_loss= 0.66366 val_acc= 0.82333 test_acc= 0.85200 time= 0.53452\n",
            "Epoch: 0347 train_loss= 0.28771 train_acc= 0.96429 val_loss= 0.66397 val_acc= 0.82333 test_acc= 0.85100 time= 0.62492\n",
            "Epoch: 0348 train_loss= 0.25604 train_acc= 0.97857 val_loss= 0.66425 val_acc= 0.82333 test_acc= 0.85000 time= 0.61669\n",
            "Epoch: 0349 train_loss= 0.28825 train_acc= 0.99286 val_loss= 0.66455 val_acc= 0.82333 test_acc= 0.85000 time= 0.52979\n",
            "Epoch: 0350 train_loss= 0.25487 train_acc= 0.97857 val_loss= 0.66485 val_acc= 0.82333 test_acc= 0.84900 time= 0.73434\n",
            "Epoch: 0351 train_loss= 0.26747 train_acc= 0.97143 val_loss= 0.66505 val_acc= 0.82333 test_acc= 0.84900 time= 0.67375\n",
            "Epoch: 0352 train_loss= 0.27420 train_acc= 0.98571 val_loss= 0.66516 val_acc= 0.82333 test_acc= 0.85100 time= 0.66809\n",
            "Epoch: 0353 train_loss= 0.27564 train_acc= 0.97143 val_loss= 0.66502 val_acc= 0.82333 test_acc= 0.85000 time= 0.61381\n",
            "Epoch: 0354 train_loss= 0.25073 train_acc= 0.96429 val_loss= 0.66477 val_acc= 0.82333 test_acc= 0.85000 time= 0.68446\n",
            "Epoch: 0355 train_loss= 0.24388 train_acc= 0.96429 val_loss= 0.66448 val_acc= 0.82333 test_acc= 0.85200 time= 0.58338\n",
            "Epoch: 0356 train_loss= 0.28428 train_acc= 0.97143 val_loss= 0.66398 val_acc= 0.82333 test_acc= 0.85200 time= 0.36906\n",
            "Epoch: 0357 train_loss= 0.27953 train_acc= 0.96429 val_loss= 0.66348 val_acc= 0.82333 test_acc= 0.85300 time= 0.36161\n",
            "Epoch: 0358 train_loss= 0.28647 train_acc= 0.96429 val_loss= 0.66306 val_acc= 0.82333 test_acc= 0.85300 time= 0.36220\n",
            "Epoch: 0359 train_loss= 0.28652 train_acc= 0.95714 val_loss= 0.66277 val_acc= 0.82333 test_acc= 0.85300 time= 0.36075\n",
            "Epoch: 0360 train_loss= 0.28049 train_acc= 0.96429 val_loss= 0.66259 val_acc= 0.82333 test_acc= 0.85300 time= 0.36406\n",
            "Epoch: 0361 train_loss= 0.28451 train_acc= 0.96429 val_loss= 0.66244 val_acc= 0.82333 test_acc= 0.85300 time= 0.37543\n",
            "Epoch: 0362 train_loss= 0.24421 train_acc= 0.97143 val_loss= 0.66211 val_acc= 0.82333 test_acc= 0.85300 time= 0.35620\n",
            "Epoch: 0363 train_loss= 0.27473 train_acc= 0.97857 val_loss= 0.66171 val_acc= 0.82333 test_acc= 0.85300 time= 0.35422\n",
            "Epoch: 0364 train_loss= 0.24109 train_acc= 0.98571 val_loss= 0.66123 val_acc= 0.82333 test_acc= 0.85400 time= 0.36485\n",
            "Epoch: 0365 train_loss= 0.24340 train_acc= 0.96429 val_loss= 0.66078 val_acc= 0.82333 test_acc= 0.85300 time= 0.36085\n",
            "Epoch: 0366 train_loss= 0.26490 train_acc= 0.96429 val_loss= 0.66038 val_acc= 0.82333 test_acc= 0.85400 time= 0.35369\n",
            "Epoch: 0367 train_loss= 0.23306 train_acc= 0.99286 val_loss= 0.65996 val_acc= 0.82333 test_acc= 0.85400 time= 0.36975\n",
            "Epoch: 0368 train_loss= 0.28317 train_acc= 0.97143 val_loss= 0.65957 val_acc= 0.82333 test_acc= 0.85400 time= 0.36286\n",
            "Epoch: 0369 train_loss= 0.28827 train_acc= 0.97857 val_loss= 0.65922 val_acc= 0.82333 test_acc= 0.85400 time= 0.36216\n",
            "Epoch: 0370 train_loss= 0.22599 train_acc= 0.98571 val_loss= 0.65873 val_acc= 0.82333 test_acc= 0.85400 time= 0.38159\n",
            "Epoch: 0371 train_loss= 0.27566 train_acc= 0.96429 val_loss= 0.65832 val_acc= 0.82667 test_acc= 0.85500 time= 0.35431\n",
            "Epoch: 0372 train_loss= 0.26859 train_acc= 0.94286 val_loss= 0.65801 val_acc= 0.82667 test_acc= 0.85500 time= 0.36111\n",
            "Epoch: 0373 train_loss= 0.26397 train_acc= 0.97857 val_loss= 0.65766 val_acc= 0.82667 test_acc= 0.85500 time= 0.37610\n",
            "Epoch: 0374 train_loss= 0.27559 train_acc= 0.97857 val_loss= 0.65724 val_acc= 0.82667 test_acc= 0.85500 time= 0.35721\n",
            "Epoch: 0375 train_loss= 0.30259 train_acc= 0.96429 val_loss= 0.65688 val_acc= 0.82667 test_acc= 0.85400 time= 0.35441\n",
            "Epoch: 0376 train_loss= 0.26078 train_acc= 0.98571 val_loss= 0.65666 val_acc= 0.82667 test_acc= 0.85400 time= 0.36150\n",
            "Epoch: 0377 train_loss= 0.25753 train_acc= 0.97857 val_loss= 0.65646 val_acc= 0.82667 test_acc= 0.85300 time= 0.69041\n",
            "Epoch: 0378 train_loss= 0.26097 train_acc= 0.97143 val_loss= 0.65619 val_acc= 0.82667 test_acc= 0.85200 time= 0.41506\n",
            "Epoch: 0379 train_loss= 0.27854 train_acc= 0.97143 val_loss= 0.65594 val_acc= 0.82667 test_acc= 0.85200 time= 0.35472\n",
            "Epoch: 0380 train_loss= 0.28236 train_acc= 0.96429 val_loss= 0.65578 val_acc= 0.82667 test_acc= 0.85200 time= 0.37213\n",
            "Epoch: 0381 train_loss= 0.27701 train_acc= 0.95714 val_loss= 0.65565 val_acc= 0.82333 test_acc= 0.85100 time= 0.35881\n",
            "Epoch: 0382 train_loss= 0.24732 train_acc= 0.97857 val_loss= 0.65546 val_acc= 0.82333 test_acc= 0.85100 time= 0.36281\n",
            "Epoch: 0383 train_loss= 0.27563 train_acc= 0.97857 val_loss= 0.65518 val_acc= 0.82333 test_acc= 0.85000 time= 0.36985\n",
            "Epoch: 0384 train_loss= 0.28139 train_acc= 0.96429 val_loss= 0.65482 val_acc= 0.82333 test_acc= 0.85100 time= 0.35296\n",
            "Epoch: 0385 train_loss= 0.25041 train_acc= 0.97857 val_loss= 0.65455 val_acc= 0.82667 test_acc= 0.85100 time= 0.35498\n",
            "Epoch: 0386 train_loss= 0.24113 train_acc= 0.98571 val_loss= 0.65417 val_acc= 0.82667 test_acc= 0.85100 time= 0.36637\n",
            "Epoch: 0387 train_loss= 0.28648 train_acc= 0.95714 val_loss= 0.65372 val_acc= 0.82667 test_acc= 0.85100 time= 0.35793\n",
            "Epoch: 0388 train_loss= 0.29537 train_acc= 0.97143 val_loss= 0.65331 val_acc= 0.82667 test_acc= 0.85200 time= 0.35178\n",
            "Epoch: 0389 train_loss= 0.25250 train_acc= 0.97857 val_loss= 0.65302 val_acc= 0.82667 test_acc= 0.85100 time= 0.35955\n",
            "Epoch: 0390 train_loss= 0.26394 train_acc= 0.97143 val_loss= 0.65287 val_acc= 0.82667 test_acc= 0.85000 time= 0.35927\n",
            "Epoch: 0391 train_loss= 0.22962 train_acc= 0.97857 val_loss= 0.65294 val_acc= 0.82667 test_acc= 0.85100 time= 0.35623\n",
            "Epoch: 0392 train_loss= 0.24736 train_acc= 0.96429 val_loss= 0.65296 val_acc= 0.82667 test_acc= 0.85100 time= 0.35967\n",
            "Epoch: 0393 train_loss= 0.22449 train_acc= 0.98571 val_loss= 0.65298 val_acc= 0.82667 test_acc= 0.85100 time= 0.35632\n",
            "Epoch: 0394 train_loss= 0.25745 train_acc= 0.95714 val_loss= 0.65304 val_acc= 0.82333 test_acc= 0.85100 time= 0.36424\n",
            "Epoch: 0395 train_loss= 0.23840 train_acc= 0.98571 val_loss= 0.65314 val_acc= 0.82333 test_acc= 0.85100 time= 0.35763\n",
            "Epoch: 0396 train_loss= 0.25382 train_acc= 0.97857 val_loss= 0.65320 val_acc= 0.82000 test_acc= 0.85200 time= 0.35937\n",
            "Epoch: 0397 train_loss= 0.25348 train_acc= 0.96429 val_loss= 0.65326 val_acc= 0.82000 test_acc= 0.85200 time= 0.36633\n",
            "Epoch: 0398 train_loss= 0.26033 train_acc= 0.97143 val_loss= 0.65341 val_acc= 0.82000 test_acc= 0.85200 time= 0.36547\n",
            "Epoch: 0399 train_loss= 0.29097 train_acc= 0.94286 val_loss= 0.65351 val_acc= 0.82000 test_acc= 0.85200 time= 0.36498\n",
            "Epoch: 0400 train_loss= 0.22461 train_acc= 0.98571 val_loss= 0.65362 val_acc= 0.82000 test_acc= 0.85100 time= 0.36536\n",
            "Epoch: 0401 train_loss= 0.26059 train_acc= 0.96429 val_loss= 0.65390 val_acc= 0.82000 test_acc= 0.85300 time= 0.36132\n",
            "Epoch: 0402 train_loss= 0.27271 train_acc= 0.97143 val_loss= 0.65417 val_acc= 0.82000 test_acc= 0.85300 time= 0.35766\n",
            "Epoch: 0403 train_loss= 0.23480 train_acc= 0.97143 val_loss= 0.65455 val_acc= 0.82000 test_acc= 0.85400 time= 0.36860\n",
            "Epoch: 0404 train_loss= 0.32163 train_acc= 0.95714 val_loss= 0.65491 val_acc= 0.82000 test_acc= 0.85400 time= 0.35213\n",
            "Epoch: 0405 train_loss= 0.23277 train_acc= 0.95714 val_loss= 0.65531 val_acc= 0.82000 test_acc= 0.85400 time= 0.35985\n",
            "Epoch: 0406 train_loss= 0.24878 train_acc= 0.98571 val_loss= 0.65561 val_acc= 0.82000 test_acc= 0.85300 time= 0.37829\n",
            "Epoch: 0407 train_loss= 0.24148 train_acc= 0.97143 val_loss= 0.65582 val_acc= 0.82000 test_acc= 0.85400 time= 0.36186\n",
            "Epoch: 0408 train_loss= 0.28635 train_acc= 0.98571 val_loss= 0.65596 val_acc= 0.82000 test_acc= 0.85400 time= 0.35559\n",
            "Epoch: 0409 train_loss= 0.23869 train_acc= 0.97143 val_loss= 0.65606 val_acc= 0.82000 test_acc= 0.85500 time= 0.37283\n",
            "Epoch: 0410 train_loss= 0.24229 train_acc= 0.97857 val_loss= 0.65621 val_acc= 0.82000 test_acc= 0.85500 time= 0.36076\n",
            "Epoch: 0411 train_loss= 0.22960 train_acc= 0.95714 val_loss= 0.65632 val_acc= 0.82000 test_acc= 0.85500 time= 0.36525\n",
            "Epoch: 0412 train_loss= 0.23485 train_acc= 0.97143 val_loss= 0.65645 val_acc= 0.82000 test_acc= 0.85500 time= 0.35813\n",
            "Epoch: 0413 train_loss= 0.28763 train_acc= 0.95714 val_loss= 0.65653 val_acc= 0.82000 test_acc= 0.85400 time= 0.35932\n",
            "Epoch: 0414 train_loss= 0.27175 train_acc= 0.97857 val_loss= 0.65664 val_acc= 0.82000 test_acc= 0.85500 time= 0.36630\n",
            "Epoch: 0415 train_loss= 0.23487 train_acc= 0.97857 val_loss= 0.65664 val_acc= 0.82000 test_acc= 0.85400 time= 0.35949\n",
            "Epoch: 0416 train_loss= 0.23500 train_acc= 0.97857 val_loss= 0.65650 val_acc= 0.82000 test_acc= 0.85300 time= 0.35893\n",
            "Epoch: 0417 train_loss= 0.21747 train_acc= 0.97857 val_loss= 0.65634 val_acc= 0.82000 test_acc= 0.85500 time= 0.36245\n",
            "Epoch: 0418 train_loss= 0.25222 train_acc= 0.96429 val_loss= 0.65599 val_acc= 0.82000 test_acc= 0.85500 time= 0.36226\n",
            "Epoch: 0419 train_loss= 0.23128 train_acc= 0.98571 val_loss= 0.65555 val_acc= 0.82000 test_acc= 0.85600 time= 0.35674\n",
            "Epoch: 0420 train_loss= 0.22653 train_acc= 0.98571 val_loss= 0.65512 val_acc= 0.82000 test_acc= 0.85600 time= 0.36113\n",
            "Epoch: 0421 train_loss= 0.23430 train_acc= 0.98571 val_loss= 0.65475 val_acc= 0.82000 test_acc= 0.85600 time= 0.36302\n",
            "Epoch: 0422 train_loss= 0.24071 train_acc= 0.98571 val_loss= 0.65444 val_acc= 0.82000 test_acc= 0.85600 time= 0.35372\n",
            "Epoch: 0423 train_loss= 0.22113 train_acc= 0.97857 val_loss= 0.65418 val_acc= 0.82000 test_acc= 0.85500 time= 0.37326\n",
            "Epoch: 0424 train_loss= 0.24706 train_acc= 0.99286 val_loss= 0.65396 val_acc= 0.82000 test_acc= 0.85300 time= 0.36179\n",
            "Epoch: 0425 train_loss= 0.21476 train_acc= 0.99286 val_loss= 0.65375 val_acc= 0.82000 test_acc= 0.85500 time= 0.36634\n",
            "Epoch: 0426 train_loss= 0.26701 train_acc= 0.96429 val_loss= 0.65372 val_acc= 0.82000 test_acc= 0.85500 time= 0.36299\n",
            "Epoch: 0427 train_loss= 0.22429 train_acc= 0.99286 val_loss= 0.65372 val_acc= 0.82000 test_acc= 0.85500 time= 0.35971\n",
            "Epoch: 0428 train_loss= 0.22668 train_acc= 0.96429 val_loss= 0.65390 val_acc= 0.82000 test_acc= 0.85500 time= 0.36850\n",
            "Epoch: 0429 train_loss= 0.22828 train_acc= 0.97857 val_loss= 0.65408 val_acc= 0.82000 test_acc= 0.85500 time= 0.36189\n",
            "Epoch: 0430 train_loss= 0.22946 train_acc= 0.99286 val_loss= 0.65416 val_acc= 0.82000 test_acc= 0.85400 time= 0.36051\n",
            "Epoch: 0431 train_loss= 0.26559 train_acc= 0.97857 val_loss= 0.65433 val_acc= 0.82000 test_acc= 0.85400 time= 0.36736\n",
            "Epoch: 0432 train_loss= 0.27258 train_acc= 0.95000 val_loss= 0.65437 val_acc= 0.82000 test_acc= 0.85400 time= 0.36737\n",
            "Epoch: 0433 train_loss= 0.25204 train_acc= 0.97143 val_loss= 0.65452 val_acc= 0.82000 test_acc= 0.85400 time= 0.37634\n",
            "Epoch: 0434 train_loss= 0.24709 train_acc= 0.96429 val_loss= 0.65470 val_acc= 0.82000 test_acc= 0.85400 time= 0.36758\n",
            "Epoch: 0435 train_loss= 0.22669 train_acc= 0.97143 val_loss= 0.65477 val_acc= 0.82000 test_acc= 0.85600 time= 0.35355\n",
            "Epoch: 0436 train_loss= 0.27170 train_acc= 0.97857 val_loss= 0.65474 val_acc= 0.82000 test_acc= 0.85600 time= 0.34632\n",
            "Epoch: 0437 train_loss= 0.23768 train_acc= 0.98571 val_loss= 0.65472 val_acc= 0.82000 test_acc= 0.85500 time= 0.37732\n",
            "Epoch: 0438 train_loss= 0.25056 train_acc= 0.96429 val_loss= 0.65486 val_acc= 0.82000 test_acc= 0.85500 time= 0.35606\n",
            "Epoch: 0439 train_loss= 0.23520 train_acc= 0.97857 val_loss= 0.65486 val_acc= 0.82000 test_acc= 0.85500 time= 0.36590\n",
            "Epoch: 0440 train_loss= 0.22755 train_acc= 0.97857 val_loss= 0.65482 val_acc= 0.82000 test_acc= 0.85500 time= 0.36994\n",
            "Epoch: 0441 train_loss= 0.22701 train_acc= 0.98571 val_loss= 0.65488 val_acc= 0.82000 test_acc= 0.85500 time= 0.35795\n",
            "Epoch: 0442 train_loss= 0.22457 train_acc= 0.98571 val_loss= 0.65484 val_acc= 0.82000 test_acc= 0.85500 time= 0.36918\n",
            "Epoch: 0443 train_loss= 0.22745 train_acc= 0.98571 val_loss= 0.65478 val_acc= 0.82000 test_acc= 0.85500 time= 0.35973\n",
            "Epoch: 0444 train_loss= 0.24509 train_acc= 0.98571 val_loss= 0.65464 val_acc= 0.82000 test_acc= 0.85600 time= 0.36139\n",
            "Epoch: 0445 train_loss= 0.22867 train_acc= 0.97143 val_loss= 0.65442 val_acc= 0.82000 test_acc= 0.85500 time= 0.36485\n",
            "Epoch: 0446 train_loss= 0.22292 train_acc= 0.98571 val_loss= 0.65421 val_acc= 0.82000 test_acc= 0.85500 time= 0.35774\n",
            "Epoch: 0447 train_loss= 0.24365 train_acc= 0.97143 val_loss= 0.65395 val_acc= 0.82000 test_acc= 0.85500 time= 0.36629\n",
            "Epoch: 0448 train_loss= 0.24729 train_acc= 0.98571 val_loss= 0.65363 val_acc= 0.82000 test_acc= 0.85300 time= 0.36851\n",
            "Epoch: 0449 train_loss= 0.21531 train_acc= 0.99286 val_loss= 0.65337 val_acc= 0.82000 test_acc= 0.85300 time= 0.35850\n",
            "Epoch: 0450 train_loss= 0.21938 train_acc= 0.97143 val_loss= 0.65307 val_acc= 0.82000 test_acc= 0.85200 time= 0.35947\n",
            "Epoch: 0451 train_loss= 0.23058 train_acc= 0.98571 val_loss= 0.65280 val_acc= 0.82000 test_acc= 0.85200 time= 0.37016\n",
            "Epoch: 0452 train_loss= 0.23632 train_acc= 0.97857 val_loss= 0.65252 val_acc= 0.82000 test_acc= 0.85200 time= 0.35425\n",
            "Epoch: 0453 train_loss= 0.20574 train_acc= 0.98571 val_loss= 0.65222 val_acc= 0.82000 test_acc= 0.85400 time= 0.35822\n",
            "Epoch: 0454 train_loss= 0.24722 train_acc= 0.97143 val_loss= 0.65185 val_acc= 0.82000 test_acc= 0.85300 time= 0.37485\n",
            "Epoch: 0455 train_loss= 0.26785 train_acc= 0.94286 val_loss= 0.65151 val_acc= 0.82000 test_acc= 0.85400 time= 0.36359\n",
            "Epoch: 0456 train_loss= 0.26609 train_acc= 0.96429 val_loss= 0.65124 val_acc= 0.82000 test_acc= 0.85400 time= 0.36565\n",
            "Epoch: 0457 train_loss= 0.25180 train_acc= 0.97857 val_loss= 0.65109 val_acc= 0.82000 test_acc= 0.85400 time= 0.36367\n",
            "Epoch: 0458 train_loss= 0.23607 train_acc= 0.97857 val_loss= 0.65094 val_acc= 0.82000 test_acc= 0.85400 time= 0.36331\n",
            "Epoch: 0459 train_loss= 0.22744 train_acc= 0.97143 val_loss= 0.65078 val_acc= 0.82000 test_acc= 0.85500 time= 0.36958\n",
            "Epoch: 0460 train_loss= 0.26191 train_acc= 0.96429 val_loss= 0.65087 val_acc= 0.82000 test_acc= 0.85400 time= 0.35054\n",
            "Epoch: 0461 train_loss= 0.23212 train_acc= 0.95000 val_loss= 0.65093 val_acc= 0.82000 test_acc= 0.85300 time= 0.36107\n",
            "Epoch: 0462 train_loss= 0.22994 train_acc= 0.97857 val_loss= 0.65107 val_acc= 0.82000 test_acc= 0.85300 time= 0.37382\n",
            "Epoch: 0463 train_loss= 0.23353 train_acc= 0.97143 val_loss= 0.65127 val_acc= 0.82000 test_acc= 0.85300 time= 0.36154\n",
            "Epoch: 0464 train_loss= 0.26239 train_acc= 0.98571 val_loss= 0.65124 val_acc= 0.82000 test_acc= 0.85400 time= 0.35388\n",
            "Epoch: 0465 train_loss= 0.23898 train_acc= 0.99286 val_loss= 0.65125 val_acc= 0.82000 test_acc= 0.85400 time= 0.37691\n",
            "Epoch: 0466 train_loss= 0.25822 train_acc= 0.97143 val_loss= 0.65115 val_acc= 0.82000 test_acc= 0.85400 time= 0.36057\n",
            "Epoch: 0467 train_loss= 0.25408 train_acc= 0.98571 val_loss= 0.65106 val_acc= 0.82000 test_acc= 0.85300 time= 0.35678\n",
            "Epoch: 0468 train_loss= 0.24757 train_acc= 0.96429 val_loss= 0.65083 val_acc= 0.82000 test_acc= 0.85300 time= 0.54345\n",
            "Epoch: 0469 train_loss= 0.24518 train_acc= 0.97143 val_loss= 0.65062 val_acc= 0.82000 test_acc= 0.85500 time= 0.54129\n",
            "Epoch: 0470 train_loss= 0.25914 train_acc= 0.96429 val_loss= 0.65042 val_acc= 0.82333 test_acc= 0.85500 time= 0.36281\n",
            "Epoch: 0471 train_loss= 0.23557 train_acc= 0.96429 val_loss= 0.65014 val_acc= 0.82333 test_acc= 0.85400 time= 0.35822\n",
            "Epoch: 0472 train_loss= 0.20963 train_acc= 0.98571 val_loss= 0.64994 val_acc= 0.82333 test_acc= 0.85400 time= 0.36392\n",
            "Epoch: 0473 train_loss= 0.21073 train_acc= 0.97143 val_loss= 0.64978 val_acc= 0.82333 test_acc= 0.85400 time= 0.35627\n",
            "Epoch: 0474 train_loss= 0.22601 train_acc= 0.99286 val_loss= 0.64959 val_acc= 0.82667 test_acc= 0.85400 time= 0.36285\n",
            "Epoch: 0475 train_loss= 0.21334 train_acc= 0.98571 val_loss= 0.64947 val_acc= 0.82667 test_acc= 0.85400 time= 0.36436\n",
            "Epoch: 0476 train_loss= 0.22268 train_acc= 0.98571 val_loss= 0.64941 val_acc= 0.82667 test_acc= 0.85300 time= 0.36172\n",
            "Epoch: 0477 train_loss= 0.23327 train_acc= 0.97143 val_loss= 0.64943 val_acc= 0.82667 test_acc= 0.85400 time= 0.35987\n",
            "Epoch: 0478 train_loss= 0.20974 train_acc= 0.97857 val_loss= 0.64946 val_acc= 0.82667 test_acc= 0.85500 time= 0.37177\n",
            "Epoch: 0479 train_loss= 0.22398 train_acc= 0.98571 val_loss= 0.64944 val_acc= 0.82667 test_acc= 0.85400 time= 0.36043\n",
            "Epoch: 0480 train_loss= 0.24645 train_acc= 0.96429 val_loss= 0.64953 val_acc= 0.82667 test_acc= 0.85400 time= 0.35683\n",
            "Epoch: 0481 train_loss= 0.25233 train_acc= 0.95714 val_loss= 0.64949 val_acc= 0.82333 test_acc= 0.85300 time= 0.37136\n",
            "Epoch: 0482 train_loss= 0.21606 train_acc= 0.98571 val_loss= 0.64946 val_acc= 0.82333 test_acc= 0.85400 time= 0.36371\n",
            "Epoch: 0483 train_loss= 0.18532 train_acc= 0.98571 val_loss= 0.64939 val_acc= 0.82333 test_acc= 0.85500 time= 0.35401\n",
            "Epoch: 0484 train_loss= 0.23016 train_acc= 0.97857 val_loss= 0.64926 val_acc= 0.82333 test_acc= 0.85400 time= 0.37736\n",
            "Epoch: 0485 train_loss= 0.21734 train_acc= 0.98571 val_loss= 0.64908 val_acc= 0.82333 test_acc= 0.85400 time= 0.36230\n",
            "Epoch: 0486 train_loss= 0.23862 train_acc= 0.97857 val_loss= 0.64887 val_acc= 0.82333 test_acc= 0.85400 time= 0.36541\n",
            "Epoch: 0487 train_loss= 0.23306 train_acc= 0.96429 val_loss= 0.64874 val_acc= 0.82333 test_acc= 0.85500 time= 0.37076\n",
            "Epoch: 0488 train_loss= 0.22878 train_acc= 0.95714 val_loss= 0.64850 val_acc= 0.82333 test_acc= 0.85500 time= 0.36118\n",
            "Epoch: 0489 train_loss= 0.19468 train_acc= 0.97857 val_loss= 0.64822 val_acc= 0.82333 test_acc= 0.85400 time= 0.37047\n",
            "Epoch: 0490 train_loss= 0.23353 train_acc= 0.97143 val_loss= 0.64802 val_acc= 0.82333 test_acc= 0.85400 time= 0.35784\n",
            "Epoch: 0491 train_loss= 0.21074 train_acc= 0.99286 val_loss= 0.64785 val_acc= 0.82333 test_acc= 0.85300 time= 0.35934\n",
            "Epoch: 0492 train_loss= 0.21145 train_acc= 0.98571 val_loss= 0.64786 val_acc= 0.82333 test_acc= 0.85400 time= 0.36771\n",
            "Epoch: 0493 train_loss= 0.23717 train_acc= 0.96429 val_loss= 0.64778 val_acc= 0.82333 test_acc= 0.85400 time= 0.36009\n",
            "Epoch: 0494 train_loss= 0.23188 train_acc= 0.97857 val_loss= 0.64768 val_acc= 0.82333 test_acc= 0.85400 time= 0.36702\n",
            "Epoch: 0495 train_loss= 0.24960 train_acc= 0.97143 val_loss= 0.64756 val_acc= 0.82333 test_acc= 0.85400 time= 0.36581\n",
            "Epoch: 0496 train_loss= 0.21990 train_acc= 0.97143 val_loss= 0.64749 val_acc= 0.82333 test_acc= 0.85300 time= 0.36952\n",
            "Epoch: 0497 train_loss= 0.23002 train_acc= 0.97143 val_loss= 0.64753 val_acc= 0.82333 test_acc= 0.85400 time= 0.36022\n",
            "Epoch: 0498 train_loss= 0.24531 train_acc= 0.98571 val_loss= 0.64760 val_acc= 0.82333 test_acc= 0.85300 time= 0.37406\n",
            "Epoch: 0499 train_loss= 0.19625 train_acc= 0.98571 val_loss= 0.64767 val_acc= 0.82333 test_acc= 0.85200 time= 0.37198\n",
            "Epoch: 0500 train_loss= 0.18919 train_acc= 0.99286 val_loss= 0.64769 val_acc= 0.82333 test_acc= 0.85200 time= 0.36288\n",
            "Epoch: 0501 train_loss= 0.19305 train_acc= 0.98571 val_loss= 0.64775 val_acc= 0.82333 test_acc= 0.85200 time= 0.36103\n",
            "Epoch: 0502 train_loss= 0.23189 train_acc= 0.97143 val_loss= 0.64780 val_acc= 0.82333 test_acc= 0.85100 time= 0.36155\n",
            "Epoch: 0503 train_loss= 0.21061 train_acc= 0.98571 val_loss= 0.64774 val_acc= 0.82333 test_acc= 0.85100 time= 0.37245\n",
            "Epoch: 0504 train_loss= 0.24207 train_acc= 0.97143 val_loss= 0.64767 val_acc= 0.82333 test_acc= 0.85100 time= 0.36833\n",
            "Epoch: 0505 train_loss= 0.21229 train_acc= 0.98571 val_loss= 0.64756 val_acc= 0.82333 test_acc= 0.85200 time= 0.36726\n",
            "Epoch: 0506 train_loss= 0.21775 train_acc= 0.97857 val_loss= 0.64742 val_acc= 0.82333 test_acc= 0.85300 time= 0.37308\n",
            "Epoch: 0507 train_loss= 0.22550 train_acc= 0.96429 val_loss= 0.64737 val_acc= 0.82333 test_acc= 0.85200 time= 0.36083\n",
            "Epoch: 0508 train_loss= 0.22453 train_acc= 0.98571 val_loss= 0.64734 val_acc= 0.82333 test_acc= 0.85200 time= 0.35551\n",
            "Epoch: 0509 train_loss= 0.25690 train_acc= 0.96429 val_loss= 0.64742 val_acc= 0.82333 test_acc= 0.85300 time= 0.37540\n",
            "Epoch: 0510 train_loss= 0.21071 train_acc= 0.97143 val_loss= 0.64751 val_acc= 0.82333 test_acc= 0.85300 time= 0.36494\n",
            "Epoch: 0511 train_loss= 0.21435 train_acc= 0.97143 val_loss= 0.64762 val_acc= 0.82333 test_acc= 0.85200 time= 0.35391\n",
            "Epoch: 0512 train_loss= 0.24883 train_acc= 0.95714 val_loss= 0.64761 val_acc= 0.82333 test_acc= 0.85300 time= 0.36744\n",
            "Epoch: 0513 train_loss= 0.23284 train_acc= 0.95714 val_loss= 0.64766 val_acc= 0.82333 test_acc= 0.85400 time= 0.36258\n",
            "Epoch: 0514 train_loss= 0.24457 train_acc= 0.98571 val_loss= 0.64778 val_acc= 0.82333 test_acc= 0.85400 time= 0.35596\n",
            "Epoch: 0515 train_loss= 0.19358 train_acc= 0.97143 val_loss= 0.64787 val_acc= 0.82333 test_acc= 0.85400 time= 0.37534\n",
            "Epoch: 0516 train_loss= 0.23152 train_acc= 0.97143 val_loss= 0.64796 val_acc= 0.82333 test_acc= 0.85400 time= 0.36282\n",
            "Epoch: 0517 train_loss= 0.22519 train_acc= 0.97143 val_loss= 0.64803 val_acc= 0.82333 test_acc= 0.85400 time= 0.37559\n",
            "Epoch: 0518 train_loss= 0.25386 train_acc= 0.96429 val_loss= 0.64818 val_acc= 0.82333 test_acc= 0.85300 time= 0.36151\n",
            "Epoch: 0519 train_loss= 0.23414 train_acc= 0.97143 val_loss= 0.64839 val_acc= 0.82333 test_acc= 0.85300 time= 0.35991\n",
            "Epoch: 0520 train_loss= 0.23990 train_acc= 0.97857 val_loss= 0.64863 val_acc= 0.82000 test_acc= 0.85300 time= 0.35674\n",
            "Epoch: 0521 train_loss= 0.21609 train_acc= 0.98571 val_loss= 0.64889 val_acc= 0.82000 test_acc= 0.85300 time= 0.35604\n",
            "Epoch: 0522 train_loss= 0.22412 train_acc= 0.97857 val_loss= 0.64917 val_acc= 0.82000 test_acc= 0.85400 time= 0.35481\n",
            "Epoch: 0523 train_loss= 0.24218 train_acc= 0.99286 val_loss= 0.64944 val_acc= 0.82000 test_acc= 0.85400 time= 0.38099\n",
            "Epoch: 0524 train_loss= 0.21789 train_acc= 0.99286 val_loss= 0.64980 val_acc= 0.82000 test_acc= 0.85400 time= 0.35870\n",
            "Epoch: 0525 train_loss= 0.22797 train_acc= 0.96429 val_loss= 0.65020 val_acc= 0.82000 test_acc= 0.85300 time= 0.36006\n",
            "Epoch: 0526 train_loss= 0.22340 train_acc= 0.97857 val_loss= 0.65055 val_acc= 0.82000 test_acc= 0.85300 time= 0.37474\n",
            "Epoch: 0527 train_loss= 0.20458 train_acc= 0.97143 val_loss= 0.65094 val_acc= 0.82000 test_acc= 0.85300 time= 0.35843\n",
            "Epoch: 0528 train_loss= 0.19220 train_acc= 0.97857 val_loss= 0.65150 val_acc= 0.82000 test_acc= 0.85300 time= 0.35831\n",
            "Epoch: 0529 train_loss= 0.23041 train_acc= 0.95714 val_loss= 0.65192 val_acc= 0.82000 test_acc= 0.85400 time= 0.37393\n",
            "Epoch: 0530 train_loss= 0.21824 train_acc= 0.96429 val_loss= 0.65256 val_acc= 0.82000 test_acc= 0.85400 time= 0.35778\n",
            "Epoch: 0531 train_loss= 0.18857 train_acc= 0.98571 val_loss= 0.65311 val_acc= 0.82000 test_acc= 0.85300 time= 0.35968\n",
            "Epoch: 0532 train_loss= 0.21564 train_acc= 0.99286 val_loss= 0.65356 val_acc= 0.82000 test_acc= 0.85300 time= 0.37546\n",
            "Epoch: 0533 train_loss= 0.21867 train_acc= 0.97857 val_loss= 0.65377 val_acc= 0.82000 test_acc= 0.85200 time= 0.35676\n",
            "Epoch: 0534 train_loss= 0.19046 train_acc= 0.98571 val_loss= 0.65398 val_acc= 0.82000 test_acc= 0.85200 time= 0.36854\n",
            "Epoch: 0535 train_loss= 0.20516 train_acc= 0.99286 val_loss= 0.65416 val_acc= 0.82000 test_acc= 0.85100 time= 0.36126\n",
            "Epoch: 0536 train_loss= 0.24149 train_acc= 0.97143 val_loss= 0.65424 val_acc= 0.82000 test_acc= 0.85100 time= 0.35646\n",
            "Epoch: 0537 train_loss= 0.22714 train_acc= 0.95714 val_loss= 0.65426 val_acc= 0.82000 test_acc= 0.85100 time= 0.36976\n",
            "Epoch: 0538 train_loss= 0.22855 train_acc= 0.97143 val_loss= 0.65430 val_acc= 0.82000 test_acc= 0.85100 time= 0.36112\n",
            "Epoch: 0539 train_loss= 0.25923 train_acc= 0.95714 val_loss= 0.65425 val_acc= 0.82000 test_acc= 0.85100 time= 0.35775\n",
            "Epoch: 0540 train_loss= 0.18955 train_acc= 0.98571 val_loss= 0.65432 val_acc= 0.82000 test_acc= 0.85100 time= 0.36839\n",
            "Epoch: 0541 train_loss= 0.20994 train_acc= 0.97857 val_loss= 0.65414 val_acc= 0.82000 test_acc= 0.85100 time= 0.35192\n",
            "Epoch: 0542 train_loss= 0.21145 train_acc= 0.97857 val_loss= 0.65399 val_acc= 0.82000 test_acc= 0.85000 time= 0.35572\n",
            "Epoch: 0543 train_loss= 0.22153 train_acc= 0.96429 val_loss= 0.65400 val_acc= 0.82000 test_acc= 0.85000 time= 0.36665\n",
            "Epoch: 0544 train_loss= 0.21876 train_acc= 0.95714 val_loss= 0.65385 val_acc= 0.82000 test_acc= 0.85000 time= 0.35944\n",
            "Epoch: 0545 train_loss= 0.20413 train_acc= 0.98571 val_loss= 0.65383 val_acc= 0.82000 test_acc= 0.85000 time= 0.35521\n",
            "Epoch: 0546 train_loss= 0.25219 train_acc= 0.95000 val_loss= 0.65396 val_acc= 0.82000 test_acc= 0.85000 time= 0.36505\n",
            "Epoch: 0547 train_loss= 0.21675 train_acc= 0.96429 val_loss= 0.65409 val_acc= 0.82000 test_acc= 0.85000 time= 0.37687\n",
            "Epoch: 0548 train_loss= 0.20813 train_acc= 0.99286 val_loss= 0.65427 val_acc= 0.82000 test_acc= 0.84900 time= 0.36630\n",
            "Epoch: 0549 train_loss= 0.22176 train_acc= 0.97857 val_loss= 0.65456 val_acc= 0.82000 test_acc= 0.84800 time= 0.36303\n",
            "Epoch: 0550 train_loss= 0.21214 train_acc= 0.98571 val_loss= 0.65469 val_acc= 0.82000 test_acc= 0.84800 time= 0.36530\n",
            "Epoch: 0551 train_loss= 0.25893 train_acc= 0.97143 val_loss= 0.65464 val_acc= 0.82000 test_acc= 0.84800 time= 0.36693\n",
            "Epoch: 0552 train_loss= 0.25902 train_acc= 0.95714 val_loss= 0.65463 val_acc= 0.82000 test_acc= 0.84800 time= 0.35075\n",
            "Epoch: 0553 train_loss= 0.23582 train_acc= 0.97143 val_loss= 0.65472 val_acc= 0.82000 test_acc= 0.84800 time= 0.35860\n",
            "Epoch: 0554 train_loss= 0.21908 train_acc= 0.97857 val_loss= 0.65476 val_acc= 0.82000 test_acc= 0.84800 time= 0.36429\n",
            "Epoch: 0555 train_loss= 0.17567 train_acc= 0.99286 val_loss= 0.65476 val_acc= 0.82000 test_acc= 0.84800 time= 0.35985\n",
            "Epoch: 0556 train_loss= 0.22606 train_acc= 0.94286 val_loss= 0.65475 val_acc= 0.82000 test_acc= 0.84800 time= 0.35250\n",
            "Epoch: 0557 train_loss= 0.20930 train_acc= 0.98571 val_loss= 0.65469 val_acc= 0.82000 test_acc= 0.85000 time= 0.36919\n",
            "Epoch: 0558 train_loss= 0.23987 train_acc= 0.96429 val_loss= 0.65469 val_acc= 0.82000 test_acc= 0.85000 time= 0.35761\n",
            "Epoch: 0559 train_loss= 0.19333 train_acc= 0.98571 val_loss= 0.65473 val_acc= 0.82000 test_acc= 0.85000 time= 0.36324\n",
            "Epoch: 0560 train_loss= 0.20478 train_acc= 0.98571 val_loss= 0.65469 val_acc= 0.82000 test_acc= 0.85000 time= 0.36552\n",
            "Epoch: 0561 train_loss= 0.24740 train_acc= 0.95714 val_loss= 0.65467 val_acc= 0.82000 test_acc= 0.85100 time= 0.35818\n",
            "Epoch: 0562 train_loss= 0.21315 train_acc= 0.97143 val_loss= 0.65480 val_acc= 0.82000 test_acc= 0.85200 time= 0.35782\n",
            "Epoch: 0563 train_loss= 0.25686 train_acc= 0.96429 val_loss= 0.65478 val_acc= 0.82000 test_acc= 0.85300 time= 0.36429\n",
            "Epoch: 0564 train_loss= 0.20840 train_acc= 0.96429 val_loss= 0.65459 val_acc= 0.82000 test_acc= 0.85300 time= 0.35521\n",
            "Epoch: 0565 train_loss= 0.20258 train_acc= 0.99286 val_loss= 0.65445 val_acc= 0.82000 test_acc= 0.85300 time= 0.35875\n",
            "Epoch: 0566 train_loss= 0.20705 train_acc= 0.97857 val_loss= 0.65426 val_acc= 0.82000 test_acc= 0.85300 time= 0.36418\n",
            "Epoch: 0567 train_loss= 0.20000 train_acc= 0.98571 val_loss= 0.65398 val_acc= 0.82000 test_acc= 0.85300 time= 0.36566\n",
            "Epoch: 0568 train_loss= 0.20736 train_acc= 0.97857 val_loss= 0.65372 val_acc= 0.82000 test_acc= 0.85300 time= 0.36375\n",
            "Epoch: 0569 train_loss= 0.23111 train_acc= 0.97857 val_loss= 0.65357 val_acc= 0.82000 test_acc= 0.85300 time= 0.36102\n",
            "Epoch: 0570 train_loss= 0.19112 train_acc= 0.97857 val_loss= 0.65330 val_acc= 0.82000 test_acc= 0.85300 time= 0.36688\n",
            "Epoch: 0571 train_loss= 0.19363 train_acc= 0.98571 val_loss= 0.65309 val_acc= 0.82000 test_acc= 0.85300 time= 0.36404\n",
            "Epoch: 0572 train_loss= 0.22342 train_acc= 0.97857 val_loss= 0.65288 val_acc= 0.82000 test_acc= 0.85400 time= 0.35713\n",
            "Epoch: 0573 train_loss= 0.20794 train_acc= 0.99286 val_loss= 0.65254 val_acc= 0.82000 test_acc= 0.85500 time= 0.35519\n",
            "Epoch: 0574 train_loss= 0.19732 train_acc= 1.00000 val_loss= 0.65222 val_acc= 0.82000 test_acc= 0.85400 time= 0.36076\n",
            "Epoch: 0575 train_loss= 0.20348 train_acc= 0.97857 val_loss= 0.65203 val_acc= 0.82000 test_acc= 0.85300 time= 0.35372\n",
            "Epoch: 0576 train_loss= 0.21683 train_acc= 0.97857 val_loss= 0.65178 val_acc= 0.82000 test_acc= 0.85300 time= 0.59946\n",
            "Epoch: 0577 train_loss= 0.19358 train_acc= 1.00000 val_loss= 0.65163 val_acc= 0.82000 test_acc= 0.85300 time= 0.37561\n",
            "Epoch: 0578 train_loss= 0.18729 train_acc= 0.97143 val_loss= 0.65162 val_acc= 0.82000 test_acc= 0.85300 time= 0.55161\n",
            "Epoch: 0579 train_loss= 0.23416 train_acc= 0.97143 val_loss= 0.65167 val_acc= 0.82000 test_acc= 0.85300 time= 0.41308\n",
            "Epoch: 0580 train_loss= 0.17522 train_acc= 0.99286 val_loss= 0.65163 val_acc= 0.82000 test_acc= 0.85300 time= 0.51750\n",
            "Epoch: 0581 train_loss= 0.20819 train_acc= 0.97857 val_loss= 0.65142 val_acc= 0.82000 test_acc= 0.85300 time= 0.36246\n",
            "Epoch: 0582 train_loss= 0.24591 train_acc= 0.96429 val_loss= 0.65123 val_acc= 0.82000 test_acc= 0.85300 time= 0.36023\n",
            "Epoch: 0583 train_loss= 0.22069 train_acc= 0.97143 val_loss= 0.65105 val_acc= 0.82000 test_acc= 0.85300 time= 0.35849\n",
            "Epoch: 0584 train_loss= 0.21502 train_acc= 0.98571 val_loss= 0.65091 val_acc= 0.82000 test_acc= 0.85300 time= 0.36542\n",
            "Epoch: 0585 train_loss= 0.21072 train_acc= 0.98571 val_loss= 0.65103 val_acc= 0.82000 test_acc= 0.85300 time= 0.35441\n",
            "Epoch: 0586 train_loss= 0.23748 train_acc= 0.96429 val_loss= 0.65119 val_acc= 0.82000 test_acc= 0.85300 time= 0.36019\n",
            "Epoch: 0587 train_loss= 0.22418 train_acc= 0.96429 val_loss= 0.65130 val_acc= 0.82000 test_acc= 0.85300 time= 0.37859\n",
            "Epoch: 0588 train_loss= 0.23023 train_acc= 0.97143 val_loss= 0.65129 val_acc= 0.82000 test_acc= 0.85300 time= 0.35786\n",
            "Epoch: 0589 train_loss= 0.20557 train_acc= 0.97143 val_loss= 0.65119 val_acc= 0.82000 test_acc= 0.85300 time= 0.37931\n",
            "Epoch: 0590 train_loss= 0.21010 train_acc= 0.99286 val_loss= 0.65095 val_acc= 0.82000 test_acc= 0.85300 time= 0.36012\n",
            "Epoch: 0591 train_loss= 0.19687 train_acc= 0.97143 val_loss= 0.65080 val_acc= 0.82000 test_acc= 0.85300 time= 0.35479\n",
            "Epoch: 0592 train_loss= 0.22249 train_acc= 0.97143 val_loss= 0.65043 val_acc= 0.82000 test_acc= 0.85400 time= 0.36268\n",
            "Epoch: 0593 train_loss= 0.23139 train_acc= 0.95714 val_loss= 0.65015 val_acc= 0.82000 test_acc= 0.85400 time= 0.35465\n",
            "Epoch: 0594 train_loss= 0.22626 train_acc= 0.95714 val_loss= 0.65008 val_acc= 0.82000 test_acc= 0.85400 time= 0.35628\n",
            "Epoch: 0595 train_loss= 0.19813 train_acc= 0.99286 val_loss= 0.65000 val_acc= 0.82000 test_acc= 0.85500 time= 0.35695\n",
            "Epoch: 0596 train_loss= 0.21303 train_acc= 0.97857 val_loss= 0.64999 val_acc= 0.82000 test_acc= 0.85500 time= 0.36006\n",
            "Epoch: 0597 train_loss= 0.20248 train_acc= 0.96429 val_loss= 0.65012 val_acc= 0.82000 test_acc= 0.85500 time= 0.35922\n",
            "Epoch: 0598 train_loss= 0.19493 train_acc= 0.98571 val_loss= 0.65006 val_acc= 0.82000 test_acc= 0.85500 time= 0.36494\n",
            "Epoch: 0599 train_loss= 0.20558 train_acc= 0.97143 val_loss= 0.65014 val_acc= 0.82000 test_acc= 0.85500 time= 0.36278\n",
            "Epoch: 0600 train_loss= 0.16205 train_acc= 0.99286 val_loss= 0.65021 val_acc= 0.82000 test_acc= 0.85500 time= 0.36807\n",
            "Epoch: 0601 train_loss= 0.22203 train_acc= 0.97143 val_loss= 0.65015 val_acc= 0.82000 test_acc= 0.85500 time= 0.36640\n",
            "Epoch: 0602 train_loss= 0.18587 train_acc= 0.98571 val_loss= 0.65020 val_acc= 0.82000 test_acc= 0.85500 time= 0.35306\n",
            "Epoch: 0603 train_loss= 0.21002 train_acc= 0.97143 val_loss= 0.65028 val_acc= 0.82000 test_acc= 0.85500 time= 0.35318\n",
            "Epoch: 0604 train_loss= 0.19352 train_acc= 0.97143 val_loss= 0.65035 val_acc= 0.82000 test_acc= 0.85500 time= 0.36477\n",
            "Epoch: 0605 train_loss= 0.18449 train_acc= 0.97857 val_loss= 0.65036 val_acc= 0.82000 test_acc= 0.85500 time= 0.35717\n",
            "Epoch: 0606 train_loss= 0.18484 train_acc= 0.97857 val_loss= 0.65041 val_acc= 0.82000 test_acc= 0.85500 time= 0.37645\n",
            "Epoch: 0607 train_loss= 0.21570 train_acc= 0.97857 val_loss= 0.65048 val_acc= 0.82000 test_acc= 0.85500 time= 0.35932\n",
            "Epoch: 0608 train_loss= 0.22736 train_acc= 0.96429 val_loss= 0.65064 val_acc= 0.82000 test_acc= 0.85500 time= 0.36286\n",
            "Optimization Finished!\n",
            "----------------------------------------------\n",
            "The finall result: 0.8519992\n",
            "----------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}